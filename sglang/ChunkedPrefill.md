# SGLang Chunked Prefill å®Œå…¨æŒ‡å—

> åŸºäº SGLang æºç çš„æ·±åº¦è§£æ

## ğŸ“š ç›®å½•

1. [ä»€ä¹ˆæ˜¯ Chunked Prefill](#1-ä»€ä¹ˆæ˜¯-chunked-prefill)
2. [Chunked Prefill vs PD åˆ†ç¦»](#2-chunked-prefill-vs-pd-åˆ†ç¦»)
3. [æ··åˆè°ƒåº¦ç­–ç•¥ï¼ˆP+Dï¼‰](#3-æ··åˆè°ƒåº¦ç­–ç•¥pd)
4. [èµ„æºåˆ†é…ä¸æ¯”ä¾‹](#4-èµ„æºåˆ†é…ä¸æ¯”ä¾‹)
5. [è§¦å‘æ¡ä»¶ä¸åˆ¤æ–­é€»è¾‘](#5-è§¦å‘æ¡ä»¶ä¸åˆ¤æ–­é€»è¾‘)
6. [æ ¸å¿ƒæ¶æ„ä¸æ–‡ä»¶](#6-æ ¸å¿ƒæ¶æ„ä¸æ–‡ä»¶)
7. [å®Œæ•´å·¥ä½œæµç¨‹](#7-å®Œæ•´å·¥ä½œæµç¨‹)
8. [å®æˆ˜ç¤ºä¾‹](#8-å®æˆ˜ç¤ºä¾‹)

---

## 1. ä»€ä¹ˆæ˜¯ Chunked Prefill

### 1.1 åŸºæœ¬æ¦‚å¿µ

**Chunked Prefill** æ˜¯ä¸€ç§å°†é•¿è¾“å…¥åºåˆ—çš„ Prefill é˜¶æ®µåˆ†æˆå¤šä¸ªå°å—ï¼ˆchunksï¼‰é€æ­¥å¤„ç†çš„ä¼˜åŒ–æŠ€æœ¯ã€‚

```
ä¼ ç»Ÿ Prefill:
è¾“å…¥ 10,000 tokens â†’ ä¸€æ¬¡æ€§å¤„ç† â†’ ç”Ÿæˆ KV cache

Chunked Prefill:
è¾“å…¥ 10,000 tokens â†’ åˆ† 5 ä¸ª chunkï¼Œæ¯ä¸ª 2,048 tokens
  â”œâ”€ Chunk 1: tokens[0:2048]
  â”œâ”€ Chunk 2: tokens[2048:4096]
  â”œâ”€ Chunk 3: tokens[4096:6144]
  â”œâ”€ Chunk 4: tokens[6144:8192]
  â””â”€ Chunk 5: tokens[8192:10000]
```

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ

LLM æ¨ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

1. **Prefill é˜¶æ®µ**ï¼šå¤„ç†æ‰€æœ‰è¾“å…¥ tokensï¼Œè®¡ç®— KV cacheï¼ˆè®¡ç®—å¯†é›†ï¼‰
2. **Decode é˜¶æ®µ**ï¼šé€ä¸ªç”Ÿæˆè¾“å‡º tokensï¼ˆå†…å­˜å¯†é›†ï¼‰

**ä¼ ç»Ÿé—®é¢˜**ï¼š
- âŒ é•¿è¾“å…¥çš„ prefill å ç”¨å¤§é‡ GPU å†…å­˜ï¼ˆæ¿€æ´»å†…å­˜ï¼‰
- âŒ Prefill è®¡ç®—é‡å¤§ï¼Œå…¶ä»–è¯·æ±‚ç­‰å¾…æ—¶é—´é•¿
- âŒ æ— æ³•ä¸ decode è¯·æ±‚æ··åˆè°ƒåº¦

**Chunked Prefill ä¼˜åŠ¿**ï¼š
- âœ… é™ä½å†…å­˜å³°å€¼ï¼šé¿å…ä¸€æ¬¡æ€§åˆ†é…å¤§é‡æ¿€æ´»å†…å­˜
- âœ… æ›´å¥½çš„æ‰¹å¤„ç†ï¼šå¯ä¸ decode è¯·æ±‚æ··åˆè°ƒåº¦
- âœ… é™ä½å»¶è¿Ÿï¼šå…¶ä»–è¯·æ±‚ä¸éœ€è¦ç­‰å¾…é•¿ prefill å®Œæˆ
- âœ… åˆ©ç”¨ Radix Cacheï¼šå·²å¤„ç†çš„ chunk å¯è¢«å…¶ä»–è¯·æ±‚å¤ç”¨

### 1.3 æ ¸å¿ƒä»£ç ä½ç½®

**åˆ¤æ–­æ˜¯å¦åˆ†å—** (`schedule_policy.py:593-638`):

```python
if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
    # Non-chunked prefill - ç›´æ¥å®Œæ•´å¤„ç†
    self.can_run_list.append(req)
    # ...
else:
    # Chunked prefill - åˆ†å—å¤„ç†
    trunc_len = self.rem_chunk_tokens // self.page_size * self.page_size
    
    # è®¾ç½®æœ¬æ¬¡å¤„ç†çš„é•¿åº¦
    req.extend_input_len = trunc_len
    req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]
    
    self.can_run_list.append(req)
    self.new_chunked_req = req  # ä¿å­˜ä¸º chunked_reqï¼Œä¸‹æ¬¡ç»§ç»­
```

**ç¼“å­˜æœªå®Œæˆçš„è¯·æ±‚** (`radix_cache_cpp.py:185-230`):

```python
def cache_unfinished_req(self, req: Req, chunked=False):
    """Cache request when it is unfinished."""
    token_ids = req.fill_ids
    prefill_len = len(token_ids)  # prefill only (maybe chunked)
    kv_indices = self.req_to_token_pool.req_to_token[req.req_pool_idx, :prefill_len]
    
    # æ’å…¥åˆ° Radix Tree
    old_prefix_len = len(req.prefix_indices) // self.page_size * self.page_size
    new_prefix_len = self._insert(RadixKey(token_ids, req.extra_key), kv_indices)
    
    # é‡æ–°åŒ¹é…å‰ç¼€ï¼Œå¤ç”¨å·²æœ‰çš„ KV cache
    new_indices_vec, _, new_last_node, _ = self.tree_cache.match_prefix(
        RadixKey(token_ids, req.extra_key).token_ids
    )
    new_indices = self._merge_tensor(new_indices_vec)
    
    # æ›´æ–°è¯·æ±‚çš„å‰ç¼€ç´¢å¼•
    if old_prefix_len < new_prefix_len:
        self.token_to_kv_pool.free(kv_indices[old_prefix_len:new_prefix_len])
        reused_indices = new_indices[old_prefix_len:new_prefix_len]
        self.req_to_token_pool.req_to_token[
            req.req_pool_idx, old_prefix_len:new_prefix_len
        ] = reused_indices
    
    req.prefix_indices = new_indices
    req.last_node = new_last_node
```

---

## 2. Chunked Prefill vs PD åˆ†ç¦»

è¿™æ˜¯ä¸¤ç§**å®Œå…¨ä¸åŒ**çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œè§£å†³ä¸åŒçš„é—®é¢˜ã€‚

### 2.1 å¯¹æ¯”è¡¨

| ç»´åº¦ | **PD åˆ†ç¦»** | **Chunked Prefill + Mixed** |
|------|------------|----------------------------|
| **è®¾è®¡ç†å¿µ** | ç‰©ç†åˆ†ç¦» | é€»è¾‘ç»Ÿåˆ |
| **éƒ¨ç½²æ¶æ„** | ä¸¤ä¸ªç‹¬ç«‹é›†ç¾¤ | å•ä¸€é›†ç¾¤ |
| **GPU ä½¿ç”¨** | åˆ†å¼€çš„ GPU | ç›¸åŒ GPU |
| **æ•°æ®ä¼ è¾“** | âœ… éœ€è¦ï¼ˆç½‘ç»œ/RDMAï¼‰ | âŒ ä¸éœ€è¦ |
| **è°ƒåº¦å™¨** | ä¸¤ä¸ªç‹¬ç«‹è°ƒåº¦å™¨ | ä¸€ä¸ªç»Ÿä¸€è°ƒåº¦å™¨ |
| **Prefill å¤„ç†** | å®Œæ•´ Prefill ä¸€æ¬¡å¤„ç† | åˆ†å—å¤„ç† |
| **æ··åˆæ‰§è¡Œ** | âŒ æ— æ³•æ··åˆ | âœ… åŒä¸€ batch æ··åˆ |
| **é€‚ç”¨åœºæ™¯** | å¤§è§„æ¨¡éƒ¨ç½²ï¼Œèµ„æºå……è¶³ | å•æœº/èµ„æºå—é™ |
| **å»¶è¿Ÿ** | æœ‰ç½‘ç»œä¼ è¾“å¼€é”€ | æ›´ä½ï¼ˆæœ¬åœ°ï¼‰ |
| **åå** | æ›´é«˜ï¼ˆä¸“é—¨ä¼˜åŒ–ï¼‰ | ä¸­ç­‰ |

### 2.2 PD åˆ†ç¦»ï¼šç‰©ç†åˆ†ç¦»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      KV Cache      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prefill é›†ç¾¤    â”‚ â”€â”€â”€â”€â”€ä¼ è¾“â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  Decode é›†ç¾¤     â”‚
â”‚  GPU 0, 1, 2    â”‚   (Mooncake/NIXL)  â”‚  GPU 3, 4, 5    â”‚
â”‚  ä¸“é—¨åš Prefill  â”‚                     â”‚  ä¸“é—¨åš Decode   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
- ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„ event loop
- éœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“ KV cache
- Prefill å’Œ Decode äº’ä¸å¹²æ‰°
```

**ä»£ç ä½ç½®** (`disaggregation/prefill.py`, `disaggregation/decode.py`)

**å¯åŠ¨ç¤ºä¾‹**:
```bash
# Prefill æœåŠ¡å™¨
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B \
  --disaggregation-mode prefill \
  --disaggregation-ib-device mlx5_0

# Decode æœåŠ¡å™¨  
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B \
  --disaggregation-mode decode \
  --port 30001 \
  --disaggregation-ib-device mlx5_0
```

### 2.3 Chunked Prefill + Mixedï¼šé€»è¾‘ç»Ÿåˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        å•ä¸€ Scheduler (åŒä¸€ GPU)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Batch 1 = [Prefill Chunk 2048t,   â”‚
â”‚             Decode Req A (1t),     â”‚
â”‚             Decode Req B (1t)]     â”‚
â”‚                                    â”‚
â”‚  åŒä¸€ä¸ª forward è°ƒç”¨å¤„ç†ï¼          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
- å•ä¸€ event loop
- æ‰€æœ‰æ•°æ®åœ¨æœ¬åœ° GPU
- Prefill chunks å’Œ Decode åœ¨åŒä¸€ batch
```

**ä»£ç ä½ç½®** (`scheduler.py:1996-2014`)

**å¯åŠ¨ç¤ºä¾‹**:
```bash
# å•ä¸€æœåŠ¡å™¨ï¼Œå¯ç”¨æ··åˆè°ƒåº¦
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.1-8B \
  --chunked-prefill-size 4096 \
  --enable-mixed-chunk  # ğŸ”¥ å…³é”®å‚æ•°
```

---

## 3. æ··åˆè°ƒåº¦ç­–ç•¥ï¼ˆP+Dï¼‰

Chunked Prefill é€šè¿‡**åˆ†å—**ä½¿å¾— Prefill å˜å¾—"è½»é‡"ï¼Œä»è€Œå¯ä»¥å’Œ Decode åœ¨åŒä¸€ä¸ª batch ä¸­æ‰§è¡Œã€‚

### 3.1 æ ¸å¿ƒæ€æƒ³

```python
# é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸€ä¸ª batch ä¸­åŒæ—¶å¤„ç† Prefill å’Œ Decodeï¼Ÿ
# ç­”æ¡ˆï¼šé€šè¿‡ extend_lens å‘Šè¯‰ attention å±‚æ¯ä¸ªè¯·æ±‚çš„å¤„ç†é•¿åº¦

Batch = {
    reqs: [Prefill_Req, Decode_A, Decode_B],
    input_ids: [p0, p1, ..., p2047, dA, dB],  # 2050 ä¸ª tokens
    extend_lens: [2048, 1, 1],  # ğŸ”‘ å…³é”®ï¼
    #            ^^^^  ^  ^
    #         Prefill  D  D
    #                  A  B
}
```

### 3.2 ä¸‰æ­¥æ··åˆç­–ç•¥

#### Step 1: èµ„æºé¢„ç•™ï¼ˆåœ¨è°ƒåº¦æ—¶ï¼‰

**ä»£ç ä½ç½®** (`scheduler.py:1871-1881`):

```python
# åˆ›å»º PrefillAdder æ—¶é¢„ç•™ decode ç©ºé—´
adder = PrefillAdder(
    self.page_size,
    self.tree_cache,
    self.token_to_kv_pool_allocator,
    self.running_batch,
    self.new_token_ratio,
    self.max_prefill_tokens,
    self.chunked_prefill_size,
    running_bs if self.is_mixed_chunk else 0,  # ğŸ”‘ é¢„ç•™ç©ºé—´
    #            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    #            å¦‚æœå¯ç”¨æ··åˆï¼Œå‡å» running batch çš„å¤§å°
    self.priority_scheduling_preemption_threshold,
)
```

**å†…éƒ¨é€»è¾‘** (`schedule_policy.py:333-336`):

```python
self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
self.rem_chunk_tokens = rem_chunk_tokens
if self.rem_chunk_tokens is not None:
    self.rem_chunk_tokens -= mixed_with_decode_tokens
    
# ä¾‹å¦‚ï¼š
# chunked_prefill_size = 2048
# mixed_with_decode_tokens = 10 (æœ‰ 10 ä¸ª decode è¯·æ±‚)
# å®é™…å¯ç”¨äº prefill: 2048 - 10 = 2038 tokens
```

#### Step 2: åˆ¤æ–­æ˜¯å¦åˆ†å—ï¼ˆæ·»åŠ  Prefill è¯·æ±‚æ—¶ï¼‰

**ä»£ç ä½ç½®** (`schedule_policy.py:593-638`):

```python
if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
    # åœºæ™¯ 1: è¾“å…¥å¾ˆçŸ­ï¼Œä¸éœ€è¦åˆ†å—
    # ä¾‹å¦‚ï¼šinput_tokens=100, rem_chunk_tokens=2038
    # 100 < 2038ï¼Œç›´æ¥å®Œæ•´å¤„ç†
    self.can_run_list.append(req)
    self._update_prefill_budget(prefix_len, input_tokens, max_new_tokens)
else:
    # åœºæ™¯ 2: è¾“å…¥å¾ˆé•¿ï¼Œéœ€è¦åˆ†å—
    # ä¾‹å¦‚ï¼šinput_tokens=10000, rem_chunk_tokens=2038
    # 10000 > 2038ï¼Œåˆ†å—å¤„ç†
    trunc_len = self.rem_chunk_tokens // self.page_size * self.page_size
    req.extend_input_len = trunc_len  # æœ¬æ¬¡åªå¤„ç† 2038 tokens
    req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]
    
    self.can_run_list.append(req)
    self.new_chunked_req = req  # ä¿å­˜ï¼Œä¸‹æ¬¡ç»§ç»­å¤„ç†å‰©ä½™éƒ¨åˆ†
```

#### Step 3: ç‰©ç†æ··åˆï¼ˆåˆ›å»º Batch æ—¶ï¼‰

**ä»£ç ä½ç½®** (`scheduler.py:1996-2014`):

```python
# åˆ›å»ºæ–°çš„ prefill batch
new_batch = ScheduleBatch.init_new(can_run_list, ...)
new_batch.prepare_for_extend()

# Mixed-style chunked prefill
if (
    self.is_mixed_chunk
    and not self.running_batch.is_empty()
    and not (new_batch.return_logprob or self.running_batch.return_logprob)
):
    # è¿‡æ»¤æ‰å·²å®Œæˆçš„ decode è¯·æ±‚
    self.running_batch.filter_batch()
    if not self.running_batch.is_empty():
        # å‡†å¤‡ decode batch
        self.running_batch.prepare_for_decode()
        
        # ğŸ”¥ å…³é”®ï¼šæ··åˆï¼
        new_batch.mix_with_running(self.running_batch)
        new_batch.decoding_reqs = self.running_batch.reqs
    
    # æ¸…ç©º running_batch
    self.running_batch = ScheduleBatch(
        reqs=[], batch_is_full=self.running_batch.batch_is_full
    )
else:
    new_batch.decoding_reqs = None

return new_batch
```

**æ··åˆå‡½æ•°** (`schedule_batch.py:1458-1486`):

```python
def mix_with_running(self, running_batch: "ScheduleBatch"):
    # 1. è®¾ç½®æ··åˆæ¨¡å¼
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()
    
    # 2. è®¾ç½® decode è¯·æ±‚çš„ extend_len = 1
    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1  # ğŸ”‘ Decode åªå¤„ç† 1 ä¸ª token
    
    # 3. æ‹¼æ¥å¼ é‡
    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    #                      ^^^^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^^
    #                      Prefill tokens   Decode tokens
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])
    
    # 4. åˆå¹¶ batch
    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc
    
    # 5. æ›´æ–° extend_lens
    delta = 0 if self.enable_overlap else -1
    self.prefix_lens.extend([
        len(r.origin_input_ids) + len(r.output_ids) + delta
        for r in running_batch.reqs
    ])
    self.extend_lens.extend([1] * running_bs)  # æ¯ä¸ª decode éƒ½æ˜¯ 1
    self.extend_num_tokens += running_bs
```

### 3.3 ForwardMode çš„ä½œç”¨

**å®šä¹‰** (`forward_batch_info.py:62-70`):

```python
class ForwardMode(IntEnum):
    EXTEND = auto()   # çº¯ Prefill
    DECODE = auto()   # çº¯ Decode
    MIXED = auto()    # ğŸ”¥ Prefill + Decode æ··åˆï¼
    IDLE = auto()
    TARGET_VERIFY = auto()
    DRAFT_EXTEND = auto()
```

Attention å±‚ä¼šæ ¹æ® `forward_mode` å’Œ `extend_lens` åˆ¤æ–­å¦‚ä½•å¤„ç†æ¯ä¸ªè¯·æ±‚ï¼š

```python
# ä¼ªä»£ç ï¼šAttention å±‚çš„é€»è¾‘
if forward_mode == ForwardMode.MIXED:
    for i, extend_len in enumerate(extend_lens):
        if extend_len > 1:
            # Prefill attention: all-to-all
            attention_prefill(tokens[offset:offset+extend_len])
        else:
            # Decode attention: attend to history
            attention_decode(tokens[offset], history_kv_cache)
        offset += extend_len
```

---

## 4. èµ„æºåˆ†é…ä¸æ¯”ä¾‹

### 4.1 D å’Œ P çš„èµ„æºå ç”¨

**ç­”æ¡ˆï¼šåœ¨è°ƒåº¦é¢„ç®—æ—¶ï¼Œç¡®å®éƒ½æŒ‰ 1 token è®¡ç®—ï¼**

```python
# schedule_policy.py:333-336
self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
if self.rem_chunk_tokens is not None:
    self.rem_chunk_tokens -= mixed_with_decode_tokens
    
# ç›´æ¥ä»å¯ç”¨ token é¢„ç®—ä¸­å‡å» decode çš„æ•°é‡ï¼Œæ²¡æœ‰ä»»ä½•æƒé‡ç³»æ•°ï¼
```

### 4.2 çœŸå®çš„ P:D Token æ¯”ä¾‹

ä» `server_args.py` çš„è‡ªåŠ¨é…ç½®æ•°æ®ï¼š

| GPU ç±»å‹ | Prefill Chunk Size | Max Decode Batch | ç†è®ºæœ€å¤§æ¯”ä¾‹ | 
|---------|-------------------|------------------|-------------|
| **T4 / RTX 4080** | 2,048 | 8 | **256:1** |
| **RTX 4090 (TP1)** | 2,048 | 16 | **128:1** |
| **RTX 4090 (TP4+)** | 2,048 | 80 | **25:1** |
| **A100 40GB (TP1)** | 4,096 | 32 | **128:1** |
| **A100 40GB (TP4+)** | 4,096 | 160 | **25:1** |
| **H100 (TP1)** | 8,192 | 256 | **32:1** |
| **H100 (TP4+)** | 8,192 | 512 | **16:1** |
| **B200 / MI300** | 16,384 | 512 | **32:1** |

**ä»£ç ä½ç½®** (`server_args.py:575-631`)

### 4.3 å®é™…è¿è¡Œç¤ºä¾‹

```python
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
åœºæ™¯ï¼šRTX 4090 (TP1)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
chunked_prefill_size = 2048
cuda_graph_max_bs = 16
running_batch æœ‰ 8 ä¸ª decode è¯·æ±‚

èµ„æºåˆ†é…ï¼š
- Prefill tokens: 2048 - 8 = 2040
- Decode tokens: 8
- å®é™…æ¯”ä¾‹: 2040:8 = 255:1

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ··åˆ Batch ç»„æˆï¼š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
new_batch = {
    reqs: [prefill_req, d1, d2, d3, d4, d5, d6, d7, d8],
    input_ids: [p0, p1, ..., p2039, d1, d2, d3, d4, d5, d6, d7, d8],
    extend_lens: [2040, 1, 1, 1, 1, 1, 1, 1, 1],
    forward_mode: ForwardMode.MIXED
}

æ€» token æ•° = 2040 + 8 = 2048 âœ“
```

### 4.4 å¯¹åº”å…³ç³»

```
N ä¸ª Decode ä»»åŠ¡ â†’ N Ã— 1 token/ä»»åŠ¡ = N ä¸ª tokens

ä¾‹å¦‚ï¼š
- 2 ä¸ª decode ä»»åŠ¡ â†’ 2 ä¸ª tokensï¼ˆæ¯ä¸ªä»»åŠ¡ 1 tokenï¼‰
- 10 ä¸ª decode ä»»åŠ¡ â†’ 10 ä¸ª tokensï¼ˆæ¯ä¸ªä»»åŠ¡ 1 tokenï¼‰
- 100 ä¸ª decode ä»»åŠ¡ â†’ 100 ä¸ª tokensï¼ˆæ¯ä¸ªä»»åŠ¡ 1 tokenï¼‰
```

æ¯ä¸ª decode è¯·æ±‚åœ¨ä¸€æ¬¡ forward ä¸­åªç”Ÿæˆ 1 ä¸ª tokenï¼Œè¿™æ˜¯ auto-regressive ç”Ÿæˆçš„æœ¬è´¨ï¼

---

## 5. è§¦å‘æ¡ä»¶ä¸åˆ¤æ–­é€»è¾‘

### 5.1 æ ¸å¿ƒåˆ¤æ–­é€»è¾‘

**åªçœ‹è¾“å…¥ tokens æ•°é‡ï¼Œä¸çœ‹è¾“å‡ºï¼**

```python
# schedule_policy.py:593
if input_tokens <= self.rem_chunk_tokens:
    # ä¸åˆ†å— - ç›´æ¥å®Œæ•´ prefill
else:
    # åˆ†å— - Chunked prefill
```

### 5.2 åœºæ™¯åˆ†æ

#### åœºæ™¯ A: é•¿æ–‡æ¡£åˆ†æï¼ˆä¼šè§¦å‘ï¼‰

```python
prompt = "è¯·åˆ†æè¿™ç¯‡10ä¸‡å­—çš„è®ºæ–‡ï¼š[10ä¸‡å­—å†…å®¹]"
input_tokens â‰ˆ 150,000 tokens
max_new_tokens = 2000
chunked_prefill_size = 2048

åˆ¤æ–­ï¼š
input_tokens (150,000) > chunked_prefill_size (2048) âœ“

ç»“æœï¼šâœ… è§¦å‘ Chunked Prefillï¼
åˆ†æˆçº¦ 73 ä¸ª chunks (150,000 / 2048)
```

#### åœºæ™¯ B: å¤šå›¾ç‰‡åˆ†æï¼ˆä¼šè§¦å‘ï¼‰

```python
prompt = "åˆ†æè¿™50å¼ å›¾ç‰‡" + [50å¼ å›¾ç‰‡]
input_tokens â‰ˆ 50 * 1000 = 50,000 tokens (æ¯å¼ å›¾ ~1k tokens)
max_new_tokens = 500
chunked_prefill_size = 4096

åˆ¤æ–­ï¼š
input_tokens (50,000) > chunked_prefill_size (4096) âœ“

ç»“æœï¼šâœ… è§¦å‘ Chunked Prefillï¼
åˆ†æˆçº¦ 12 ä¸ª chunks
```

#### åœºæ™¯ C: çŸ­ prompt + é•¿è¾“å‡ºï¼ˆ**ä¸**è§¦å‘ï¼‰

```python
prompt = "ç»™æˆ‘å†™ä¸ª5000å­—çš„æ•£æ–‡"
input_tokens â‰ˆ 10 tokens
max_new_tokens = 7500
chunked_prefill_size = 2048

åˆ¤æ–­ï¼š
input_tokens (10) < chunked_prefill_size (2048) âœ“

ç»“æœï¼šâŒ ä¸è§¦å‘ Chunked Prefill
Prefill ä¸€æ¬¡å®Œæˆï¼Œç„¶åè¿›å…¥é•¿æ—¶é—´çš„ Decode é˜¶æ®µ
```

### 5.3 è®¾è®¡å“²å­¦

Chunked Prefill æ˜¯ä¸º**é•¿è¾“å…¥**åœºæ™¯è®¾è®¡çš„ï¼š
- ğŸ“„ é•¿æ–‡æ¡£ç†è§£
- ğŸ–¼ï¸ å¤šå›¾ç‰‡è§†è§‰ä»»åŠ¡
- ğŸ’¬ è¶…é•¿å¯¹è¯å†å²

å¯¹äºçŸ­è¾“å…¥é•¿è¾“å‡ºçš„ç”Ÿæˆä»»åŠ¡ï¼Œå®ƒä¸ä»‹å…¥ï¼Œè®©ä¼ ç»Ÿ continuous batching è‡ªç„¶å·¥ä½œã€‚

### 5.4 å®Œæ•´æµç¨‹ç¤ºä¾‹

```python
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
åœºæ™¯ï¼šé•¿æ–‡æ¡£åˆ†æ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è¾“å…¥ï¼š150,000 tokens çš„æ–‡æ¡£
chunked_prefill_size = 2048
running_batch = 10 ä¸ª decode è¯·æ±‚

Step 1: ç¬¬ä¸€è½®è°ƒåº¦
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
rem_chunk_tokens = 2048 - 10 = 2038
input_tokens = 150,000
input_tokens > rem_chunk_tokens âœ“ â†’ è§¦å‘åˆ†å—

å¤„ç†ï¼štokens[0:2038]
Batch = {
    reqs: [doc_req, d1, d2, ..., d10],
    extend_lens: [2038, 1, 1, ..., 1],
    forward_mode: MIXED
}
å‰©ä½™ï¼š150,000 - 2038 = 147,962 tokens

Step 2: ç¬¬äºŒè½®è°ƒåº¦
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
rem_chunk_tokens = 2048 - 10 = 2038
å‰©ä½™ input_tokens = 147,962
input_tokens > rem_chunk_tokens âœ“ â†’ ç»§ç»­åˆ†å—

å¤„ç†ï¼štokens[2038:4076]
å‰©ä½™ï¼š147,962 - 2038 = 145,924 tokens

...é‡å¤çº¦ 73 è½®...

Step 73: æœ€åä¸€è½®
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å‰©ä½™ input_tokens = 1500
input_tokens < rem_chunk_tokens âœ“ â†’ å®Œæˆåˆ†å—

å¤„ç†ï¼štokens[148,500:150,000]
chunked_req = None  # å®Œæˆ

Step 74: å¼€å§‹ Decode
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Prefill å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆ 2000 tokens çš„è¾“å‡º
```

---

## 6. æ ¸å¿ƒæ¶æ„ä¸æ–‡ä»¶

### 6.1 æ•´ä½“æ¶æ„

```
ç”¨æˆ·è¯·æ±‚
   â†“
server_args.py (é…ç½®å±‚)
   â†“
scheduler.py (è°ƒåº¦æ ¸å¿ƒ)
   â”œâ”€â”€ schedule_policy.py (ç­–ç•¥å±‚)
   â”œâ”€â”€ schedule_batch.py (æ•°æ®å±‚)
   â””â”€â”€ å¾ªç¯è°ƒåº¦
```

### 6.2 æ–‡ä»¶èŒè´£

#### 1. `server_args.py` - é…ç½®ä¸­å¿ƒ

**ä½œç”¨**ï¼šå®šä¹‰ SGLang å¯åŠ¨çš„æ‰€æœ‰å‚æ•°

**å…³é”®é…ç½®**ï¼š
```python
@dataclasses.dataclass
class ServerArgs:
    # Chunked Prefill ç›¸å…³
    chunked_prefill_size: Optional[int] = None  # chunk å¤§å°
    enable_mixed_chunk: bool = False            # æ˜¯å¦æ··åˆè°ƒåº¦
    max_prefill_tokens: int = 16384            # æœ€å¤§ prefill tokens
    
    # è‡ªåŠ¨é…ç½®
    def _handle_gpu_memory_settings(self, gpu_mem):
        if gpu_mem < 20 * 1024:
            self.chunked_prefill_size = 2048
        elif gpu_mem < 35 * 1024:
            self.chunked_prefill_size = 2048
        elif gpu_mem < 60 * 1024:
            self.chunked_prefill_size = 4096
        # ...
```

#### 2. `scheduler.py` - è°ƒåº¦æ ¸å¿ƒ

**ä½œç”¨**ï¼šSGLang çš„"å¤§è„‘"ï¼Œè´Ÿè´£æ•´ä¸ªæ¨ç†æµç¨‹

**æ ¸å¿ƒæ–¹æ³•**ï¼š
```python
class Scheduler:
    def event_loop_normal(self):
        """ä¸»äº‹ä»¶å¾ªç¯"""
        while True:
            recv_reqs = self.recv_requests()
            self.process_input_requests(recv_reqs)
            batch = self.get_next_batch_to_run()
            if batch:
                result = self.run_batch(batch)
                self.process_batch_result(batch, result)
    
    def get_new_batch_prefill(self) -> ScheduleBatch:
        """åˆ›å»º prefill batchï¼ˆChunked Prefill å…¥å£ï¼‰"""
        adder = PrefillAdder(...)
        
        # å¤„ç†ä¸Šä¸€è½®çš„ chunked_req
        if self.chunked_req is not None:
            self.chunked_req.init_next_round_input()
            self.chunked_req = adder.add_chunked_req(self.chunked_req)
        
        # æ·»åŠ æ–°è¯·æ±‚
        for req in self.waiting_queue:
            adder.add_one_req(req, ...)
        
        # åˆ›å»º batch
        new_batch = ScheduleBatch.init_new(adder.can_run_list, ...)
        
        # æ··åˆè°ƒåº¦
        if self.is_mixed_chunk and not self.running_batch.is_empty():
            new_batch.mix_with_running(self.running_batch)
        
        return new_batch
```

#### 3. `schedule_batch.py` - æ•°æ®ç»“æ„å±‚

**ä½œç”¨**ï¼šå®šä¹‰è¯·æ±‚å’Œæ‰¹æ¬¡çš„æ•°æ®ç»“æ„

**æ ¸å¿ƒç±»**ï¼š
```python
class Req:
    """å•ä¸ªè¯·æ±‚"""
    origin_input_ids: List[int]  # åŸå§‹è¾“å…¥
    output_ids: List[int]        # å·²ç”Ÿæˆè¾“å‡º
    prefix_indices: torch.Tensor # ç¼“å­˜çš„ KV ç´¢å¼•
    extend_input_len: int        # æœ¬æ¬¡å¤„ç†é•¿åº¦ï¼ˆChunked Prefillï¼‰
    fill_ids: List[int]          # å½“å‰éœ€è¦å¡«å……çš„ IDs

class ScheduleBatch:
    """æ‰¹æ¬¡"""
    reqs: List[Req]
    forward_mode: ForwardMode    # EXTEND / DECODE / MIXED
    prefix_lens: List[int]       # æ¯ä¸ªè¯·æ±‚çš„å‰ç¼€é•¿åº¦
    extend_lens: List[int]       # æ¯ä¸ªè¯·æ±‚çš„æ‰©å±•é•¿åº¦
    decoding_reqs: List[Req]     # æ··åˆæ¨¡å¼ä¸‹çš„ decode è¯·æ±‚
    
    def mix_with_running(self, running_batch):
        """æ··åˆ Prefill å’Œ Decode"""
        self.forward_mode = ForwardMode.MIXED
        for req in running_batch.reqs:
            req.extend_input_len = 1  # Decode åªå¤„ç† 1 token
        self.input_ids = torch.cat([self.input_ids, running_batch.input_ids])
        self.extend_lens.extend([1] * running_batch.batch_size())
```

#### 4. `schedule_policy.py` - ç­–ç•¥å†³ç­–å±‚

**ä½œç”¨**ï¼šå†³å®šè¯·æ±‚çš„è°ƒåº¦é¡ºåºå’Œèµ„æºåˆ†é…

**æ ¸å¿ƒç±»**ï¼š
```python
class SchedulePolicy:
    """æ’åºç­–ç•¥"""
    def calc_priority(self, waiting_queue: List[Req]):
        # FCFS, LPM, LOF, DFS_WEIGHT, RANDOM
        pass

class PrefillAdder:
    """èµ„æºé¢„ç®—ç®¡ç†ï¼ˆæœ€é‡è¦ï¼ï¼‰"""
    def __init__(self, ..., mixed_with_decode_tokens: int = 0):
        # é¢„ç•™ decode ç©ºé—´
        self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens
        self.rem_chunk_tokens = rem_chunk_tokens - mixed_with_decode_tokens
    
    def add_one_req(self, req: Req):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—"""
        if input_tokens <= self.rem_chunk_tokens:
            # ä¸åˆ†å—
            self.can_run_list.append(req)
        else:
            # åˆ†å—
            trunc_len = self.rem_chunk_tokens // self.page_size * self.page_size
            req.extend_input_len = trunc_len
            self.new_chunked_req = req
    
    def add_chunked_req(self, req: Req):
        """ç»§ç»­å¤„ç†åˆ†å—è¯·æ±‚"""
        req.extend_input_len = min(req.extend_input_len, self.rem_chunk_tokens)
        return req if truncated else None
```

### 6.3 å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   server_args.py                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ServerArgs                                  â”‚    â”‚
â”‚  â”‚ - chunked_prefill_size: 2048-16384         â”‚    â”‚
â”‚  â”‚ - enable_mixed_chunk: True/False           â”‚    â”‚
â”‚  â”‚ - cuda_graph_max_bs: 8-512                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   scheduler.py                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Scheduler                                   â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  event_loop_normal()                        â”‚    â”‚
â”‚  â”‚    â”œâ”€ recv_requests()                       â”‚    â”‚
â”‚  â”‚    â”œâ”€ get_next_batch_to_run()              â”‚    â”‚
â”‚  â”‚    â”‚    â””â”€ get_new_batch_prefill() â—„â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚    â”œâ”€ run_batch()                        â”‚  â”‚    â”‚
â”‚  â”‚    â””â”€ process_batch_result()             â”‚  â”‚    â”‚
â”‚  â”‚                                           â”‚  â”‚    â”‚
â”‚  â”‚  chunked_req: Req                        â”‚  â”‚    â”‚
â”‚  â”‚  is_mixed_chunk: bool                    â”‚  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜  â”‚    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”¼â”€â”€â”€â”€â”€â”˜    â”‚
                                         â”‚ â”‚          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
                  â†“                        â†“          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”
â”‚    schedule_policy.py       â”‚  â”‚  schedule_batch.py    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ SchedulePolicy     â”‚     â”‚  â”‚  â”‚ Req              â”‚ â”‚
â”‚  â”‚ - calc_priority()  â”‚     â”‚  â”‚  â”‚ - fill_ids       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  â”‚ - prefix_indices â”‚ â”‚
â”‚                             â”‚  â”‚  â”‚ - extend_input_lenâ”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ PrefillAdder       â”‚     â”‚  â”‚                       â”‚
â”‚  â”‚ - add_one_req()    â”‚â—„â”€â”€â”€â”€â”¼â”€â”€â”¤  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ - add_chunked_req()â”‚     â”‚  â”‚  â”‚ ScheduleBatch    â”‚ â”‚
â”‚  â”‚ - rem_chunk_tokens â”‚     â”‚  â”‚  â”‚ - forward_mode   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  â”‚ - extend_lens    â”‚ â”‚
â”‚                             â”‚  â”‚  â”‚ - mix_with_running()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. å®Œæ•´å·¥ä½œæµç¨‹

### 7.1 å•æ¬¡ Chunked Prefill æµç¨‹

```python
# ========================================
# 1. ç”¨æˆ·è¯·æ±‚åˆ°è¾¾
# ========================================
ç”¨æˆ·å‘é€: "åˆ†æè¿™ç¯‡ 10 ä¸‡å­—çš„è®ºæ–‡ï¼š[æ–‡æœ¬]"
â†’ Tokenizer: ç”Ÿæˆ 150,000 tokens

# ========================================
# 2. åˆ›å»º Req å¯¹è±¡
# ========================================
req = Req(
    rid="req_123",
    origin_input_ids=[token_0, token_1, ..., token_149999],  # 150k tokens
    sampling_params=SamplingParams(max_new_tokens=2000),
    ...
)
waiting_queue.append(req)

# ========================================
# 3. Scheduler äº‹ä»¶å¾ªç¯
# ========================================
def event_loop_normal():
    while True:
        # 3.1 è·å–ä¸‹ä¸€ä¸ª batch
        batch = get_next_batch_to_run()
        
        # 3.2 å¦‚æœéœ€è¦ prefill
        if need_prefill:
            batch = get_new_batch_prefill()

# ========================================
# 4. åˆ›å»º Prefill Batch
# ========================================
def get_new_batch_prefill():
    # 4.1 æ’åºç­–ç•¥
    policy.calc_priority(waiting_queue)
    
    # 4.2 åˆ›å»º PrefillAdder
    running_bs = len(running_batch.reqs)  # å‡è®¾ 10 ä¸ª decode
    adder = PrefillAdder(
        page_size=1,
        tree_cache=tree_cache,
        token_to_kv_pool_allocator=token_to_kv_pool,
        running_batch=running_batch,
        new_token_ratio=0.4,
        rem_input_tokens=16384,
        rem_chunk_tokens=2048,
        mixed_with_decode_tokens=10 if is_mixed_chunk else 0,  # ğŸ”‘
    )
    # adder.rem_chunk_tokens = 2048 - 10 = 2038
    
    # 4.3 å¤„ç†ä¹‹å‰çš„ chunked_req
    if chunked_req is not None:
        chunked_req.init_next_round_input()
        chunked_req = adder.add_chunked_req(chunked_req)
    
    # 4.4 æ·»åŠ æ–°è¯·æ±‚
    for req in waiting_queue:
        req.init_next_round_input(tree_cache)
        # åŒ¹é…å‰ç¼€
        match_result = tree_cache.match_prefix(req.origin_input_ids)
        req.prefix_indices = match_result.device_indices  # å‡è®¾ 0ï¼ˆæ— ç¼“å­˜ï¼‰
        req.extend_input_len = len(req.origin_input_ids)  # 150,000
        
        # åˆ¤æ–­æ˜¯å¦åˆ†å—
        result = adder.add_one_req(req, has_chunked_req=False, ...)
        # input_tokens=150,000 > rem_chunk_tokens=2038 â†’ åˆ†å—ï¼
        # req.extend_input_len = 2038
        # req.fill_ids = origin_input_ids[0:2038]
        # adder.new_chunked_req = req
        
        break  # åªæ·»åŠ ä¸€ä¸ªè¯·æ±‚
    
    # 4.5 åˆ›å»º batch
    new_batch = ScheduleBatch.init_new(
        reqs=adder.can_run_list,  # [req]
        ...
    )
    new_batch.prepare_for_extend()
    # new_batch.input_ids = [token_0, ..., token_2037]
    # new_batch.extend_lens = [2038]
    
    # 4.6 æ··åˆè°ƒåº¦
    if is_mixed_chunk and not running_batch.is_empty():
        running_batch.filter_batch()  # è¿‡æ»¤å®Œæˆçš„è¯·æ±‚
        running_batch.prepare_for_decode()
        
        new_batch.mix_with_running(running_batch)
        # new_batch.input_ids = [token_0, ..., token_2037, d1, d2, ..., d10]
        # new_batch.extend_lens = [2038, 1, 1, ..., 1]
        # new_batch.forward_mode = ForwardMode.MIXED
    
    return new_batch

# ========================================
# 5. è¿è¡Œ Batch
# ========================================
result = run_batch(new_batch)
# GPU forward:
#   - Prefill: tokens[0:2038]
#   - Decode: 10 ä¸ªè¯·æ±‚å„ç”Ÿæˆ 1 token

# ========================================
# 6. å¤„ç†ç»“æœ
# ========================================
process_batch_result(new_batch, result)
# 6.1 ç¼“å­˜ chunked_req
tree_cache.cache_unfinished_req(chunked_req, chunked=True)
# 6.2 æ›´æ–° chunked_req
chunked_req.fill_ids = origin_input_ids  # æ¢å¤å®Œæ•´è¾“å…¥
chunked_req.extend_input_len = 150,000 - 2038  # å‰©ä½™éƒ¨åˆ†

# ========================================
# 7. ä¸‹ä¸€è½®è°ƒåº¦
# ========================================
# chunked_req ä¼šåœ¨ä¸‹ä¸€è½®ç»§ç»­å¤„ç†
# é‡å¤æ­¥éª¤ 4-6ï¼Œç›´åˆ°æ‰€æœ‰ 150k tokens å¤„ç†å®Œæ¯•
```

### 7.2 å¤šè½® Chunked Prefill æ—¶é—´çº¿

```
æ—¶é—´çº¿ï¼š
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Round 1:  [Prefill 2038t] [D1 D2 ... D10]  â†’ å¤„ç† tokens[0:2038]
            å‰©ä½™ï¼š147,962 tokens
            
Round 2:  [Prefill 2038t] [D1 D2 ... D10]  â†’ å¤„ç† tokens[2038:4076]
            å‰©ä½™ï¼š145,924 tokens
            
Round 3:  [Prefill 2038t] [D1 D2 ... D10]  â†’ å¤„ç† tokens[4076:6114]
            å‰©ä½™ï¼š143,886 tokens
            
...é‡å¤çº¦ 73 è½®...

Round 73: [Prefill 1500t] [D1 D2 ... D10]  â†’ å¤„ç† tokens[148,500:150,000]
            Prefill å®Œæˆï¼
            
Round 74: [D1 D2 ... D10]                  â†’ å¼€å§‹ç”Ÿæˆè¾“å‡º
Round 75: [D1 D2 ... D10]
...
Round 2073: [D1]                           â†’ æœ€åä¸€ä¸ªè¯·æ±‚å®Œæˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 7.3 å…³é”®ä»£ç è°ƒç”¨é“¾

```
1. scheduler.py:event_loop_normal()
   â””â”€> 2. scheduler.py:get_next_batch_to_run()
       â””â”€> 3. scheduler.py:get_new_batch_prefill()
           â”œâ”€> 4. schedule_policy.py:PrefillAdder.__init__()
           â”‚   â””â”€> rem_chunk_tokens -= mixed_with_decode_tokens
           â”‚
           â”œâ”€> 5. schedule_policy.py:add_one_req()
           â”‚   â””â”€> if input_tokens > rem_chunk_tokens:
           â”‚       â”œâ”€> req.extend_input_len = trunc_len
           â”‚       â””â”€> self.new_chunked_req = req
           â”‚
           â”œâ”€> 6. schedule_batch.py:ScheduleBatch.init_new()
           â”‚   â””â”€> åˆ›å»º batch å¯¹è±¡
           â”‚
           â””â”€> 7. schedule_batch.py:mix_with_running()
               â”œâ”€> self.forward_mode = ForwardMode.MIXED
               â”œâ”€> req.extend_input_len = 1 (for decode)
               â”œâ”€> input_ids = torch.cat([prefill, decode])
               â””â”€> extend_lens.extend([1] * running_bs)
```

---

## 8. å®æˆ˜ç¤ºä¾‹

### 8.1 å¯åŠ¨é…ç½®

```bash
# ç¤ºä¾‹ 1: RTX 4090 å•å¡ï¼Œå¯ç”¨æ··åˆè°ƒåº¦
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3-8B \
  --chunked-prefill-size 2048 \
  --enable-mixed-chunk \
  --cuda-graph-max-bs 16 \
  --port 30000

# ç¤ºä¾‹ 2: H100 å•å¡ï¼Œæ›´å¤§çš„ chunk
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3-70B \
  --chunked-prefill-size 8192 \
  --enable-mixed-chunk \
  --cuda-graph-max-bs 256 \
  --tp-size 4

# ç¤ºä¾‹ 3: ç¦ç”¨ chunked prefill
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3-8B \
  --chunked-prefill-size -1  # -1 è¡¨ç¤ºç¦ç”¨
```

### 8.2 æ—¥å¿—è§£è¯»

```
# å¯åŠ¨æ—¥å¿—
[2025-01-10 10:00:00] max_total_num_tokens=665690, chunked_prefill_size=2048, 
                       max_prefill_tokens=16384, max_running_requests=4096, 
                       context_len=65536, available_gpu_mem=13.50 GB

# Prefill batch æ—¥å¿—
[2025-01-10 10:01:00] Prefill batch. #new-seq: 1, #new-token: 2000, 
                       #cached-token: 0, token usage: 0.15, 
                       #running-req: 20, #queue-req: 5

è§£è¯»ï¼š
- #new-seq: 1              â†’ 1 ä¸ªæ–°è¯·æ±‚
- #new-token: 2000         â†’ æœ¬æ¬¡ prefill å¤„ç† 2000 tokens (chunked)
- #cached-token: 0         â†’ æ— ç¼“å­˜å‘½ä¸­
- token usage: 0.15        â†’ KV cache ä½¿ç”¨ç‡ 15%
- #running-req: 20         â†’ æœ‰ 20 ä¸ª decode è¯·æ±‚æ­£åœ¨è¿è¡Œ
- #queue-req: 5            â†’ ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰ 5 ä¸ªè¯·æ±‚

å®é™…æ¯”ä¾‹: 2000 prefill : 20 decode = 100:1

# Decode batch æ—¥å¿—
[2025-01-10 10:01:01] Decode batch. #running-req: 21, #token: 12500, 
                       token usage: 0.18
```

### 8.3 æ€§èƒ½å¯¹æ¯”

```python
# åœºæ™¯ï¼šå¤„ç† 100k token çš„é•¿æ–‡æ¡£

# ä¸ä½¿ç”¨ Chunked Prefill
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ—¶é—´    æ“ä½œ
0s      Prefill 100k tokens (é˜»å¡æ‰€æœ‰å…¶ä»–è¯·æ±‚)
15s     Prefill å®Œæˆï¼Œå¼€å§‹ Decode
        å…¶ä»– 20 ä¸ªè¯·æ±‚åœ¨ç­‰å¾…é˜Ÿåˆ—ä¸­é¥¿æ­»
        TTFT (Time To First Token): 15s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ä½¿ç”¨ Chunked Prefill (2048 chunk)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ—¶é—´    æ“ä½œ
0s      Round 1: Prefill 2048t + Decode 20 reqs
0.3s    Round 2: Prefill 2048t + Decode 20 reqs
0.6s    Round 3: Prefill 2048t + Decode 20 reqs
...
15s     Round 49: Prefill å®Œæˆï¼Œç»§ç»­ Decode
        å…¶ä»– 20 ä¸ªè¯·æ±‚æŒç»­ç”Ÿæˆï¼Œæ²¡æœ‰é¥¿æ­»
        TTFT (å¹³å‡): 0.5s (å…¶ä»–è¯·æ±‚)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

æ€§èƒ½æå‡ï¼š
- å…¶ä»–è¯·æ±‚çš„ TTFT: 15s â†’ 0.5s (30x æ”¹å–„)
- ååé‡æå‡: çº¦ 2-3x
- å†…å­˜å³°å€¼: é™ä½çº¦ 60%
```

### 8.4 æœ€ä½³å®è·µ

```python
# 1. æ ¹æ® GPU å†…å­˜é€‰æ‹©åˆé€‚çš„ chunk size
GPU Memory < 24GB  â†’ chunked_prefill_size=2048
GPU Memory 40GB    â†’ chunked_prefill_size=4096
GPU Memory 80GB+   â†’ chunked_prefill_size=8192

# 2. æ ¹æ®å·¥ä½œè´Ÿè½½é€‰æ‹©æ˜¯å¦å¯ç”¨æ··åˆè°ƒåº¦
é•¿è¾“å…¥ + é«˜å¹¶å‘    â†’ --enable-mixed-chunk
çŸ­è¾“å…¥ä¸ºä¸»         â†’ ä¸å¯ç”¨æ··åˆï¼ˆé»˜è®¤ï¼‰

# 3. é…åˆ Radix Cache ä½¿ç”¨
--disable-radix-cache=False  # å¯ç”¨ Radix Cache
â†’ å·²å¤„ç†çš„ chunks å¯ä»¥è¢«å…¶ä»–è¯·æ±‚å¤ç”¨

# 4. ç›‘æ§å…³é”®æŒ‡æ ‡
- token usage: åº”ä¿æŒåœ¨ 60-80%
- #running-req: åº”æ¥è¿‘ max_running_requests çš„ 70-80%
- TTFT (Time To First Token): åº” < 1s
- ITL (Inter Token Latency): åº” < 50ms
```

### 8.5 è°ƒè¯•æŠ€å·§

```python
# 1. æŸ¥çœ‹æ˜¯å¦è§¦å‘ Chunked Prefill
grep "Chunked prefill" /path/to/sglang.log

# 2. æŸ¥çœ‹ chunk å¤§å°
grep "chunked_prefill_size" /path/to/sglang.log

# 3. æŸ¥çœ‹æ··åˆè°ƒåº¦ç»Ÿè®¡
grep "Prefill batch" /path/to/sglang.log | grep "#running-req"
# å¦‚æœ #running-req > 0ï¼Œè¯´æ˜æ··åˆè°ƒåº¦ç”Ÿæ•ˆ

# 4. æŸ¥çœ‹å†…å­˜ä½¿ç”¨
grep "available_gpu_mem" /path/to/sglang.log

# 5. ä½¿ç”¨ Python å®¢æˆ·ç«¯æµ‹è¯•
import openai
client = openai.Client(base_url="http://localhost:30000/v1", api_key="EMPTY")

response = client.completions.create(
    model="meta-llama/Llama-3-8B",
    prompt="[å¾ˆé•¿çš„è¾“å…¥æ–‡æœ¬ï¼Œ150k tokens]",
    max_tokens=100,
    stream=True
)

for chunk in response:
    print(chunk.choices[0].text, end="", flush=True)
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Chunked Prefill** æ˜¯å°†é•¿è¾“å…¥åˆ†å—å¤„ç†çš„æŠ€æœ¯ï¼Œé™ä½å†…å­˜å³°å€¼
2. **ä¸ PD åˆ†ç¦»ä¸åŒ**ï¼šPD åˆ†ç¦»æ˜¯ç‰©ç†åˆ†ç¦»ï¼ŒChunked Prefill æ˜¯é€»è¾‘ç»Ÿåˆ
3. **æ··åˆè°ƒåº¦**ï¼šé€šè¿‡ `ForwardMode.MIXED` å’Œ `extend_lens` å®ç° P+D åŒ batch
4. **èµ„æºåˆ†é…**ï¼šP å’Œ D æŒ‰ 1 token è®¡ç®—ï¼Œå®é™…æ¯”ä¾‹çº¦ 16:1 åˆ° 256:1
5. **è§¦å‘æ¡ä»¶**ï¼šåªçœ‹è¾“å…¥ tokensï¼Œä¸çœ‹è¾“å‡ºï¼ˆ`input_tokens > chunked_prefill_size`ï¼‰
6. **æ ¸å¿ƒæ–‡ä»¶**ï¼š
   - `server_args.py`: é…ç½®
   - `scheduler.py`: è°ƒåº¦æ ¸å¿ƒ
   - `schedule_policy.py`: ç­–ç•¥å’Œé¢„ç®—
   - `schedule_batch.py`: æ•°æ®ç»“æ„

### é€‚ç”¨åœºæ™¯

âœ… **é€‚åˆ Chunked Prefill**:
- é•¿æ–‡æ¡£ç†è§£
- å¤šå›¾ç‰‡/è§†é¢‘åˆ†æ
- è¶…é•¿å¯¹è¯å†å²
- é«˜å¹¶å‘æœåŠ¡

âŒ **ä¸é€‚åˆ Chunked Prefill**:
- çŸ­è¾“å…¥é•¿è¾“å‡ºï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰
- ä½å¹¶å‘åœºæ™¯
- è¿½æ±‚æè‡´ååé‡ï¼ˆç”¨ PD åˆ†ç¦»ï¼‰

### é…ç½®å»ºè®®

```bash
# æ¨èé…ç½®ï¼ˆå¹³è¡¡æ€§èƒ½å’Œå»¶è¿Ÿï¼‰
python -m sglang.launch_server \
  --model-path YOUR_MODEL \
  --chunked-prefill-size 4096 \
  --enable-mixed-chunk \
  --cuda-graph-max-bs 32 \
  --max-running-requests 256
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-01-10  
**åŸºäº**: SGLang commit `latest`

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åé¦ˆï¼ğŸ‰

