# SGLang å†…å­˜æ± ç³»ç»Ÿå®Œæ•´ç¬”è®°

> **ä½œè€…**: AIåŠ©æ‰‹  
> **æ—¥æœŸ**: 2025-10-13  
> **ç‰ˆæœ¬**: v1.0  
> **åŸºäº**: SGLang memory_pool.py, allocator.py, memory_pool_host.py

---

## ğŸ“š ç›®å½•

1. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
2. [memory_pool.py - ç‰©ç†å­˜å‚¨å±‚](#memory_poolpy---ç‰©ç†å­˜å‚¨å±‚)
   - [ReqToTokenPool](#reqtotokenpool---è¯·æ±‚åˆ°tokenä½ç½®æ˜ å°„æ± )
   - [MambaPool](#mambapool---mambaçŠ¶æ€ç¼“å­˜æ± )
   - [HybridReqToTokenPool](#hybridreqtotokenpool---æ··åˆè¯·æ±‚tokenæ± )
   - [KVCacheç³»åˆ—](#kvcacheç³»åˆ—---ç‰©ç†å­˜å‚¨å±‚)
3. [allocator.py - åˆ†é…å™¨å±‚](#allocatorpy---åˆ†é…å™¨å±‚)
4. [memory_pool_host.py - Hostå†…å­˜å±‚](#memory_pool_hostpy---hostå†…å­˜å±‚)
5. [è®¾è®¡æ¨¡å¼æ€»ç»“](#è®¾è®¡æ¨¡å¼æ€»ç»“)
6. [æ€§èƒ½ä¼˜åŒ–æ€»ç»“](#æ€§èƒ½ä¼˜åŒ–æ€»ç»“)

---

## æ•´ä½“æ¶æ„

SGLang ä½¿ç”¨**å¤šå±‚å†…å­˜æ± **è®¾è®¡ï¼Œæ”¯æŒä»GPUåˆ°åˆ†å¸ƒå¼å­˜å‚¨çš„å®Œæ•´KV cacheå±‚æ¬¡ç»“æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¯·æ±‚å±‚ (Request)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ReqToTokenPool / HybridReqToTokenPool (memory_pool)   â”‚
â”‚   ä½œç”¨ï¼šè¯·æ±‚ â†’ Tokenä½ç½®æ˜ å°„ + MambaçŠ¶æ€ç®¡ç†            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TokenToKVPoolAllocator / PagedAllocator (allocator)   â”‚
â”‚   ä½œç”¨ï¼šTokenä½ç½® â†’ KV Cacheç´¢å¼•åˆ†é…ç­–ç•¥                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        KVCacheç³»åˆ— (memory_pool)                         â”‚
â”‚   L1 (GPU): MHATokenToKVPool, MLATokenToKVPool, ...    â”‚
â”‚   ä½œç”¨ï¼šç‰©ç†KV cacheå­˜å‚¨ï¼ˆGPUæ˜¾å­˜ï¼‰                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        HostKVCache (memory_pool_host)                   â”‚
â”‚   L2 (CPU): MHATokenToKVPoolHost, MLATokenToKVPoolHost â”‚
â”‚   ä½œç”¨ï¼šä¸»æœºå†…å­˜KV cacheï¼ˆHiCache L2å±‚ï¼‰                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        HiCacheStorage (hicache_storage)                 â”‚
â”‚   L3 (åˆ†å¸ƒå¼): Mooncake, 3FS, NIXL, etc.               â”‚
â”‚   ä½œç”¨ï¼šåˆ†å¸ƒå¼å­˜å‚¨åç«¯ï¼ˆHiCache L3å±‚ï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## memory_pool.py - ç‰©ç†å­˜å‚¨å±‚

### ReqToTokenPool - è¯·æ±‚åˆ°Tokenä½ç½®æ˜ å°„æ± 

#### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
class ReqToTokenPool:
    """ç®¡ç†è¯·æ±‚åˆ°tokenä½ç½®çš„æ˜ å°„"""
    
    # æ ¸å¿ƒå¼ é‡ï¼š[è¯·æ±‚æ•°, æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦]
    self.req_to_token: torch.Tensor  # shape: (size, max_context_len)
    # ä¾‹å¦‚ï¼š
    # req_to_token[0] = [101, 102, 103, ...]  # è¯·æ±‚0çš„tokenä½ç½®
    # req_to_token[1] = [201, 202, 203, ...]  # è¯·æ±‚1çš„tokenä½ç½®
    
    self.free_slots: List[int]  # ç©ºé—²è¯·æ±‚æ§½ä½åˆ—è¡¨
```

#### æ ¸å¿ƒæ–¹æ³•

```python
def __init__(self, size, max_context_len, device, enable_memory_saver):
    """
    Args:
        size: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆå¦‚1024ï¼‰
        max_context_len: å•ä¸ªè¯·æ±‚æœ€å¤§tokenæ•°ï¼ˆå¦‚32768ï¼‰
    """
    self.req_to_token = torch.zeros(
        (size, max_context_len), 
        dtype=torch.int32,
        device=device
    )
    self.free_slots = list(range(size))

def alloc(self, need_size: int) -> List[int]:
    """åˆ†é…è¯·æ±‚æ§½ä½"""
    if need_size > len(self.free_slots):
        return None
    select_index = self.free_slots[:need_size]
    self.free_slots = self.free_slots[need_size:]
    return select_index

def write(self, indices, values):
    """å†™å…¥tokenä½ç½®æ˜ å°„"""
    self.req_to_token[indices] = values

def free(self, free_index: Union[int, List[int]]):
    """é‡Šæ”¾æ§½ä½"""
    if isinstance(free_index, int):
        self.free_slots.append(free_index)
    else:
        self.free_slots.extend(free_index)
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
# åˆå§‹åŒ–
pool = ReqToTokenPool(size=256, max_context_len=32768, device="cuda:0")

# åˆ†é…3ä¸ªè¯·æ±‚æ§½ä½
req_slots = pool.alloc(3)  # è¿”å›: [0, 1, 2]

# ä¸ºè¯·æ±‚å†™å…¥tokenä½ç½®
pool.write(indices=[0], values=torch.tensor([[100,101,102,...]]))

# é‡Šæ”¾
pool.free([0, 1, 2])
```

---

### MambaPool - MambaçŠ¶æ€ç¼“å­˜æ± 

#### Mambaæ¶æ„èƒŒæ™¯

```
ä¼ ç»ŸTransformer: O(nÂ²) å¤æ‚åº¦
    Q @ K^T â†’ éœ€è¦å­˜å‚¨å®Œæ•´çš„KV cache

Mamba/Mamba2: O(n) å¤æ‚åº¦  
    ä½¿ç”¨å¾ªç¯çŠ¶æ€æ›´æ–° â†’ åªéœ€å­˜å‚¨å›ºå®šå¤§å°çš„çŠ¶æ€
```

#### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
class MambaPool:
    """ä¸ºMamba/Mamba2æ¶æ„å­˜å‚¨çŠ¶æ€ç©ºé—´æ¨¡å‹çš„çŠ¶æ€"""
    
    self.mamba_cache: Tuple[torch.Tensor, ...] = (
        conv_state,      # å·ç§¯çŠ¶æ€
        temporal_state,  # æ—¶åº/SSMçŠ¶æ€
        # å¦‚æœå¯ç”¨æ¨æµ‹è§£ç ï¼Œè¿˜æœ‰ï¼š
        intermediate_ssm_state_cache,
        intermediate_conv_window_cache,
    )
```

#### çŠ¶æ€å½¢çŠ¶è¯¦è§£

```python
# å·ç§¯çŠ¶æ€ conv_state
shape: (num_mamba_layers, size+1, dim, kernel_size-1)
# ä¾‹å¦‚ï¼š(24å±‚, 1025ä¸ªæ§½, 2048ç»´åº¦, 3ä¸ªå†å²è¾“å…¥)
# ä½œç”¨ï¼šä¿å­˜å·ç§¯çš„æ»‘åŠ¨çª—å£å†å²

# æ—¶åºçŠ¶æ€ temporal_state (SSM state)
shape: (num_mamba_layers, size+1, num_heads, state_dim)
# ä¾‹å¦‚ï¼š(24å±‚, 1025ä¸ªæ§½, 8ä¸ªå¤´, 64ç»´çŠ¶æ€)
# ä½œç”¨ï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹çš„éšè—çŠ¶æ€
```

#### æ¨æµ‹è§£ç çš„ä¸­é—´ç¼“å­˜

```python
if speculative_num_draft_tokens is not None:
    # ä¸­é—´SSMçŠ¶æ€ç¼“å­˜
    shape: (num_layers, size+1, num_draft_tokens, head, state_dim)
    intermediate_ssm_state_cache = torch.zeros(...)
    
    # ä¸­é—´å·ç§¯çª—å£ç¼“å­˜  
    shape: (num_layers, size+1, num_draft_tokens, dim, kernel_size-1)
    intermediate_conv_window_cache = torch.zeros(...)
```

#### æ ¸å¿ƒæ–¹æ³•

```python
def alloc(self, need_size: int) -> Optional[List[int]]:
    """åˆ†é…MambaçŠ¶æ€æ§½ä½"""
    if need_size > len(self.free_slots):
        return None
    select_index = self.free_slots[:need_size]
    self.free_slots = self.free_slots[need_size:]
    return select_index

def free(self, free_index: Union[int, List[int]]):
    """é‡Šæ”¾å¹¶æ¸…é›¶çŠ¶æ€ï¼ˆé¿å…æ±¡æŸ“ï¼‰"""
    if isinstance(free_index, int):
        self.free_slots.append(free_index)
    else:
        self.free_slots.extend(free_index)
    # æ¸…é›¶è¢«é‡Šæ”¾æ§½ä½çš„çŠ¶æ€
    self.mamba_cache[0][:, free_index] = 0  # conv_state
    self.mamba_cache[1][:, free_index] = 0  # temporal_state

def get_mamba_params(self, layer_id: int):
    """è·å–æŒ‡å®šå±‚çš„Mambaå‚æ•°"""
    return [self.mamba_cache[i][layer_id] for i in range(len(self.mamba_cache))]
```

#### å†…å­˜å ç”¨ç¤ºä¾‹

```python
# é…ç½®ï¼š24ä¸ªMambaå±‚, 1024ä¸ªå¹¶å‘è¯·æ±‚
# conv_state: (2048, 3), fp32
# temporal_state: (8, 64), fp32

conv_state_size = 24 * 1024 * 2048 * 3 * 4 / GB â‰ˆ 0.57 GB
temporal_state_size = 24 * 1024 * 8 * 64 * 4 / GB â‰ˆ 0.048 GB
æ€»è®¡ â‰ˆ 0.62 GB
```

---

### HybridReqToTokenPool - æ··åˆè¯·æ±‚Tokenæ± 

ç”¨äº **Hybrid GDN** æ¨¡å‹ï¼ˆå¦‚Qwen3Nextã€FalconH1ï¼‰ï¼ŒåŒæ—¶ç®¡ç†Tokenä½ç½®å’ŒMambaçŠ¶æ€ã€‚

#### æ ¸å¿ƒæ•°æ®ç»“æ„

```python
class HybridReqToTokenPool(ReqToTokenPool):
    """åŒæ—¶ç®¡ç†æ™®é€štokenä½ç½®å’ŒMambaç¼“å­˜"""
    
    # ç»§æ‰¿åŸºç¡€çš„ req_to_token æ˜ å°„
    self.req_to_token: torch.Tensor
    
    # æ·»åŠ  Mamba ä¸“ç”¨ç»„ä»¶
    self.mamba_pool: MambaPool
    
    # æ˜ å°„è¡¨ï¼šMambaå±‚åœ¨æ¨¡å‹ä¸­çš„å…¨å±€ID â†’ åœ¨mamba_poolä¸­çš„å±€éƒ¨ID
    self.mamba_map: Dict[int, int]
    # ä¾‹å¦‚ï¼š{5: 0, 10: 1, 15: 2} è¡¨ç¤ºç¬¬5/10/15å±‚æ˜¯Mambaå±‚
    
    # è¯·æ±‚ç´¢å¼•åˆ°Mambaç´¢å¼•çš„æ˜ å°„
    self.req_index_to_mamba_index_mapping: torch.Tensor
    
    # åŒå‘æ˜ å°„ï¼šè¯·æ±‚ID â†” Mambaç´¢å¼•
    self.rid_to_mamba_index_mapping: Dict[str, int]
    self.mamba_index_to_rid_mapping: Dict[int, str]
```

#### æ™ºèƒ½åˆ†é…ï¼šæ”¯æŒChunk Prefill

```python
def alloc(self, need_size: int, reqs: Optional[List["Req"]] = None):
    """åŒæ—¶åˆ†é…è¯·æ±‚æ§½å’ŒMambaæ§½"""
    
    # Step 1: åˆ†é…è¯·æ±‚æ§½
    select_index = super().alloc(need_size)
    if select_index == None:
        return None
    
    # Step 2: ä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…æˆ–å¤ç”¨Mambaæ§½
    mamba_index = []
    for req in reqs:
        rid = req.rid
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰Mambaæ§½ï¼ˆæ”¯æŒchunk prefillï¼‰
        if rid in self.rid_to_mamba_index_mapping:
            mid = self.rid_to_mamba_index_mapping[rid]
            # å¤ç”¨å·²æœ‰çš„MambaçŠ¶æ€ï¼
        else:
            # åˆ†é…æ–°çš„Mambaæ§½
            mid = self.mamba_pool.alloc(1)[0]
            # å»ºç«‹åŒå‘æ˜ å°„
            self.rid_to_mamba_index_mapping[rid] = mid
            self.mamba_index_to_rid_mapping[mid] = rid
        
        mamba_index.append(mid)
    
    # Step 3: è®°å½•æ˜ å°„å…³ç³»
    self.req_index_to_mamba_index_mapping[select_index] = \
        torch.tensor(mamba_index, dtype=torch.int32, device=self.device)
    
    return select_index
```

#### Chunk Prefill ç¤ºä¾‹

```python
# åœºæ™¯ï¼šé•¿åºåˆ—åˆ†å—é¢„å¡«å……

# ç¬¬ä¸€æ¬¡ï¼šå¤„ç†å‰1024ä¸ªtoken
req = Req(rid="req_001", input_ids=[0:1024])
req_idx = pool.alloc(1, [req])  
# â†’ åˆ†é… req_slot=0, mamba_slot=5

# ç¬¬äºŒæ¬¡ï¼šå¤„ç†æ¥ä¸‹æ¥çš„1024ä¸ªtoken
req = Req(rid="req_001", input_ids=[1024:2048])
req_idx = pool.alloc(1, [req])
# â†’ åˆ†é…æ–°çš„ req_slot=1, ä½†å¤ç”¨ mamba_slot=5
# è¿™æ ·MambaçŠ¶æ€ä¿æŒè¿ç»­ï¼
```

#### æ¡ä»¶é‡Šæ”¾

```python
def free(self, free_index, free_mamba_cache: bool = True):
    """å¯é€‰åœ°ä¿ç•™MambaçŠ¶æ€"""
    # é‡Šæ”¾è¯·æ±‚æ§½
    super().free(free_index)
    
    if free_mamba_cache:
        # é‡Šæ”¾Mambaæ§½å¹¶æ¸…ç†æ˜ å°„
        mamba_index = self.req_index_to_mamba_index_mapping[free_index]
        self.mamba_pool.free(mamba_index.tolist())
        # æ¸…ç†åŒå‘æ˜ å°„
        for mid in mamba_index.tolist():
            rid = self.mamba_index_to_rid_mapping.pop(mid)
            self.rid_to_mamba_index_mapping.pop(rid)
    
    # å¦‚æœ free_mamba_cache=Falseï¼ŒMambaçŠ¶æ€ä¿ç•™
    # ç”¨äºchunk prefillçš„åç»­æ‰¹æ¬¡
```

---

### KVCacheç³»åˆ— - ç‰©ç†å­˜å‚¨å±‚

#### 1. KVCache (æŠ½è±¡åŸºç±»)

```python
class KVCache(abc.ABC):
    """æ‰€æœ‰KVç¼“å­˜çš„æŠ½è±¡åŸºç±»"""
    
    # åŸºç¡€å±æ€§
    self.size: int              # æ€»tokenå®¹é‡
    self.page_size: int         # é¡µå¤§å°
    self.dtype: torch.dtype     # è®¡ç®—æ•°æ®ç±»å‹
    self.store_dtype: torch.dtype  # å­˜å‚¨æ•°æ®ç±»å‹ï¼ˆå¯èƒ½æ˜¯fp8ï¼‰
    self.layer_num: int
    
    # HiCacheæ”¯æŒ
    self.layer_transfer_counter: Optional[LayerDoneCounter]
```

**å…³é”®è®¾è®¡ï¼šdtype vs store_dtype**

```python
if dtype in (torch.float8_e5m2, torch.float8_e4m3fn):
    # FP8é‡åŒ–ï¼šè®¡ç®—ç”¨fp8ï¼Œä½†å­˜å‚¨ç”¨uint8
    # åŸå› ï¼šPyTorchçš„index_putæ“ä½œä¸æ”¯æŒfp8
    self.store_dtype = torch.uint8
else:
    self.store_dtype = dtype

# å†™å…¥æ—¶ï¼š
cache_k = cache_k.view(torch.uint8)
self.k_buffer[loc] = cache_k

# è¯»å–æ—¶ï¼š
k = self.k_buffer[loc].view(torch.float8_e4m3fn)
```

#### 2. MHATokenToKVPool - å¤šå¤´æ³¨æ„åŠ›KVæ± 

**å†…å­˜å¸ƒå±€**

```python
class MHATokenToKVPool(KVCache):
    """æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›çš„KVç¼“å­˜æ± """
    
    # Kå’ŒVåˆ†å¼€å­˜å‚¨ï¼Œæ¯å±‚ä¸€ä¸ªtensor
    self.k_buffer: List[torch.Tensor]  # é•¿åº¦ = layer_num
    self.v_buffer: List[torch.Tensor]  # é•¿åº¦ = layer_num
    
    # æ¯ä¸ªtensorçš„å½¢çŠ¶ï¼š[size + page_size, head_num, head_dim]
    # ä¾‹å¦‚ï¼š[32769, 32, 128]
    #   - 32768ä¸ªtokenæ§½ + 1ä¸ªpaddingæ§½
    #   - 32ä¸ªæ³¨æ„åŠ›å¤´
    #   - æ¯ä¸ªå¤´128ç»´
```

**slot 0çš„ç‰¹æ®Šç”¨é€”**

```python
# slot 0ç”¨äºå¸æ”¶padding tokençš„è¾“å‡º

# åœºæ™¯ï¼šæ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—
# Seq 1: [1, 2, 3]         (3 tokens)
# Seq 2: [4, 5, 6, 7, 8]   (5 tokens)

# éœ€è¦paddingåˆ°ç›¸åŒé•¿åº¦ï¼š
# Seq 1: [1, 2, 3, PAD, PAD]
# Seq 2: [4, 5, 6, 7, 8]

# Forwardæ—¶ï¼ŒPAD tokenä¹Ÿä¼šäº§ç”ŸKVè¾“å‡º
# ä½†è¿™äº›è¾“å‡ºæ˜¯æ— ç”¨çš„ï¼Œå…¨éƒ¨å†™å…¥slot 0
# è¿™æ ·å°±ä¸å ç”¨æœ‰æ•ˆçš„cacheç©ºé—´
```

**åŒæµä¼˜åŒ–å†™å…¥**

```python
def set_kv_buffer(self, layer, loc, cache_k, cache_v, ...):
    if get_is_capture_mode() and self.alt_stream is not None:
        # CUDA Graphæ¨¡å¼ï¼šä½¿ç”¨åŒæµé‡å Kå’ŒVçš„æ‹·è´
        current_stream = self.device_module.current_stream()
        
        # ä¸»æµå†™K
        self.alt_stream.wait_stream(current_stream)
        self.k_buffer[layer_id][loc] = cache_k
        
        # å¤‡ç”¨æµå†™Vï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
        with self.device_module.stream(self.alt_stream):
            self.v_buffer[layer_id][loc] = cache_v
        
        # åŒæ­¥ç­‰å¾…Vå†™å®Œ
        current_stream.wait_stream(self.alt_stream)

# æ€§èƒ½æå‡ï¼š
# å•æµæ¨¡å¼ï¼š[å†™K] â†’ [å†™V] = 200us
# åŒæµæ¨¡å¼ï¼š[å†™K] + [å†™Vå¹¶è¡Œ] â‰ˆ 110us
```

**å†…å­˜å ç”¨è®¡ç®—**

```python
# é…ç½®ï¼š32å±‚, 32768 tokens, 32å¤´Ã—128ç»´, bf16
æ¯å±‚K = 32768 * 32 * 128 * 2 bytes = 256 MB
æ¯å±‚V = 256 MB
æ¯å±‚æ€»è®¡ = 512 MB
32å±‚æ€»è®¡ = 16 GB
```

#### 3. MLATokenToKVPool - å¤šæ½œåœ¨æ³¨æ„åŠ›KVæ± 

**MLAæ ¸å¿ƒåŸç†ï¼ˆç”¨äºDeepSeek-V2/V3ï¼‰**

```python
# ä¼ ç»ŸMHAï¼š
k_buffer: [32768, 32, 128]  # 32ä¸ªå¤´Ã—128ç»´ = 4096ç»´/token
v_buffer: [32768, 32, 128]
æ€»ç»´åº¦ï¼š32 * 128 * 2 = 8192 ç»´/token

# MLAå‹ç¼©ï¼š
kv_buffer: [32768, 1, 576]  # åªæœ‰1ä¸ª"å¤´"ï¼Œ576ç»´
# compressed_dim = kv_lora_rank + qk_rope_head_dim
#                = 512 + 64 = 576
æ€»ç»´åº¦ï¼š576 ç»´/token

# å‹ç¼©æ¯”ï¼š576 / 8192 â‰ˆ 7%ï¼ŒèŠ‚çœ93%ï¼
```

**KV bufferå†…éƒ¨å¸ƒå±€**

```python
# kv_buffer[layer_id] å½¢çŠ¶ï¼š[seq_len, 1, 576]
# å†…éƒ¨ç»„ç»‡ï¼š
# [0:512]   â†’ kv_lora_rank éƒ¨åˆ†ï¼ˆVçš„ä½ç§©è¡¨ç¤ºï¼‰
# [512:576] â†’ qk_rope_head_dim éƒ¨åˆ†ï¼ˆRoPEä½ç½®ç¼–ç ï¼‰

# è®¿é—®æ—¶ï¼š
k_buffer = kv_buffer  # æ•´ä¸ª576ç»´
v_buffer = kv_buffer[..., :512]  # å‰512ç»´
```

**Triton Kernelä¼˜åŒ–æ‹¼æ¥**

```python
def set_mla_kv_buffer(self, layer, loc, cache_k_nope, cache_k_rope):
    """
    MLAçš„Kåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š
    - k_nope: æ— ä½ç½®ä¿¡æ¯ (kv_lora_rankç»´)
    - k_rope: æœ‰ä½ç½®ä¿¡æ¯ (qk_rope_head_dimç»´)
    
    ä½¿ç”¨Triton kernelé«˜æ•ˆæ‹¼æ¥
    """
    set_mla_kv_buffer_triton(
        self.kv_buffer[layer_id],
        loc,
        cache_k_nope,  # [num_tokens, 1, 512]
        cache_k_rope,  # [num_tokens, 1, 64]
    )
    # è¾“å‡ºï¼škv_buffer[loc] = [k_nope | k_rope]

# å†…å­˜èŠ‚çœï¼š
# DeepSeek-V2: 60å±‚Ã—128K tokens
# MHA: 60 GB
# MLA: 8.6 GB (èŠ‚çœ86%)
```

#### 4. SWAKVPool - æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æ··åˆæ± 

**ç”¨äºLlama4æ··åˆæ¨¡å‹**

```python
class SWAKVPool(KVCache):
    """åˆ†ç¦»ç®¡ç†SWAå±‚å’Œå…¨æ³¨æ„åŠ›å±‚çš„KV cache"""
    
    # Llama4æ¶æ„ï¼šæ¯4å±‚ä¸­ï¼Œ3å±‚ç”¨SWAï¼Œ1å±‚ç”¨å…¨æ³¨æ„åŠ›
    # å±‚0-3ï¼š  SWA, SWA, SWA, Full
    # å±‚4-7ï¼š  SWA, SWA, SWA, Full
    # ...
    
    self.swa_kv_pool: MHATokenToKVPool    # SWAå±‚çš„å°ç¼“å­˜
    self.full_kv_pool: MHATokenToKVPool   # å…¨æ³¨æ„åŠ›å±‚çš„å¤§ç¼“å­˜
    
    self.layers_mapping: Dict[int, Tuple[int, bool]]
    # æ ¼å¼ï¼š{å…¨å±€å±‚ID: (æ± å†…å±€éƒ¨ID, æ˜¯å¦æ˜¯SWAå±‚)}
    
    self.full_to_swa_index_mapping: torch.Tensor
    # å…¨æ³¨æ„åŠ›ç´¢å¼• â†’ SWAç´¢å¼•çš„æ˜ å°„
```

**æ™ºèƒ½è·¯ç”±**

```python
def get_key_buffer(self, layer_id: int):
    layer_id_pool, is_swa = self.layers_mapping[layer_id]
    
    if is_swa:
        return self.swa_kv_pool.get_key_buffer(layer_id_pool)
    else:
        return self.full_kv_pool.get_key_buffer(layer_id_pool)
```

**ç´¢å¼•ç©ºé—´è½¬æ¢**

```python
def set_kv_buffer(self, layer, loc, cache_k, cache_v, ...):
    layer_id_pool, is_swa = self.layers_mapping[layer.layer_id]
    
    if is_swa:
        # å…³é”®ï¼šlocå¯èƒ½æ˜¯å…¨æ³¨æ„åŠ›ç©ºé—´çš„ç´¢å¼•ï¼Œéœ€è¦è½¬æ¢
        if self.full_to_swa_index_mapping is not None:
            loc = self.translate_loc_from_full_to_swa(loc)
        
        self.swa_kv_pool.set_kv_buffer(None, loc, cache_k, cache_v, ...)
    else:
        self.full_kv_pool.set_kv_buffer(None, loc, cache_k, cache_v, ...)

# ç´¢å¼•è½¬æ¢ç¤ºä¾‹ï¼š
# allocatoråˆ†é…ï¼šfull_indices=[1000,1001,1002], swa_indices=[10,11,12]
# æ˜ å°„ï¼šfull_to_swa_index_mapping[1000]=10, [1001]=11, [1002]=12
# å†™å…¥SWAå±‚æ—¶è‡ªåŠ¨è½¬æ¢
```

**å†…å­˜ä¼˜åŠ¿**

```python
# é…ç½®ï¼š32å±‚(24å±‚SWA+8å±‚Full), SWAçª—å£4K, Full 128K, bf16

# ä¼ ç»Ÿæ–¹æ¡ˆï¼ˆæ‰€æœ‰å±‚éƒ½æ˜¯å…¨æ³¨æ„åŠ›ï¼‰ï¼š
traditional_size = 32 * 131072 * 32 * 128 * 2 / GB â‰ˆ 32 GB

# SWAæ··åˆæ–¹æ¡ˆï¼š
swa_size = 24 * 4096 * 32 * 128 * 2 / GB â‰ˆ 0.75 GB
full_size = 8 * 131072 * 32 * 128 * 2 / GB â‰ˆ 8 GB
total_size = 0.75 + 8 = 8.75 GB

# èŠ‚çœï¼š73%ï¼
```

#### 5. HybridLinearKVPool - æ··åˆçº¿æ€§KVæ± 

**ç”¨äºHybrid GDNæ¨¡å‹ï¼ˆQwen3Nextã€FalconH1ï¼‰**

```python
class HybridLinearKVPool(KVCache):
    """åªä¸ºå…¨æ³¨æ„åŠ›å±‚åˆ†é…KV cache"""
    
    # æ¨¡å‹ç»“æ„ï¼š
    # - éƒ¨åˆ†å±‚æ˜¯Transformerï¼ˆéœ€è¦KV cacheï¼‰
    # - éƒ¨åˆ†å±‚æ˜¯Mambaï¼ˆä¸éœ€è¦KV cacheï¼Œç”¨MambaPoolï¼‰
    
    self.full_kv_pool: MHATokenToKVPool
    self.full_attention_layer_id_mapping: Dict[int, int]
    # å…¨å±€å±‚ID â†’ æ± å†…å±€éƒ¨ID
```

**å±‚IDæ˜ å°„**

```python
# Qwen3Next 32å±‚ï¼š
# Mambaå±‚ï¼š0,1,2,3,8,9,10,11,16,17,18,19,24,25,26,27
# å…¨æ³¨æ„åŠ›å±‚ï¼š4,5,6,7,12,13,14,15,20,21,22,23,28,29,30,31

self.full_attention_layer_id_mapping = {
    4: 0, 5: 1, 6: 2, 7: 3,      # å…¨å±€4-7 â†’ å±€éƒ¨0-3
    12: 4, 13: 5, 14: 6, 15: 7,  # å…¨å±€12-15 â†’ å±€éƒ¨4-7
    20: 8, 21: 9, 22: 10, 23: 11,
    28: 12, 29: 13, 30: 14, 31: 15,
}

# åªéœ€åˆ†é…16å±‚KV cacheï¼ˆè€Œä¸æ˜¯32å±‚ï¼‰
# èŠ‚çœ50%æ˜¾å­˜ï¼
```

#### 6. NSATokenToKVPool - NSAæ³¨æ„åŠ›KVæ± 

**NSA = MLA + ç¨€ç–ç´¢å¼•ä¼˜åŒ–**

```python
class NSATokenToKVPool(MLATokenToKVPool):
    """åœ¨MLAåŸºç¡€ä¸Šæ·»åŠ index_kç¼“å­˜"""
    
    # ä¸»KV cacheï¼šå‹ç¼©çš„KVè¡¨ç¤ºï¼ˆç»§æ‰¿è‡ªMLAï¼‰
    self.kv_buffer: List[torch.Tensor]
    
    # é¢å¤–çš„ç´¢å¼•ç¼“å­˜ï¼ˆFP8é‡åŒ–ï¼‰
    self.index_k_with_scale_buffer: List[torch.Tensor]
    # å½¢çŠ¶ï¼š[num_pages, page_size*head_dim + page_size*4]
    #       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    #        FP8ç´¢å¼•æ•°æ®      FP32é‡åŒ–scale
```

**index_k_with_scale_buffer å¸ƒå±€**

```python
# page_size=64, index_head_dim=128
# æ¯ä¸ªpageï¼š
page_data_size = 64 * 128 + 64 * 4 = 8192 + 256 = 8448 bytes

# è¯¦ç»†å¸ƒå±€ï¼š
# [0:8192]    â†’ 64ä¸ªtokençš„index_k (FP8)
# [8192:8448] â†’ 64ä¸ªé‡åŒ–scale (FP32)
```

**NSAå·¥ä½œæµç¨‹**

```python
# Prefillï¼šè®¡ç®—å¹¶å­˜å‚¨index_k
for token in input_tokens:
    index_k = model.compute_index_k(token)  # [1, 128]
    index_k_fp8, scale = quantize_to_fp8(index_k)
    pool.set_index_k_and_scale_buffer(layer_id, loc, index_k_fp8, scale)

# Decodeï¼šä½¿ç”¨index_kåšç¨€ç–æ£€ç´¢
query_index = model.compute_index_k(new_token)
all_index_k = pool.get_index_k_continuous(layer_id, seq_len, page_indices)

# è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„Kä¸ªtoken
scores = query_index @ all_index_k.T
topk_indices = torch.topk(scores, k=top_k).indices

# åªå¯¹topkçš„tokenåšå®Œæ•´æ³¨æ„åŠ›
selected_kv = pool.get_kv_buffer(layer_id)[topk_indices]
output = sparse_attention(query, selected_kv)

# å†…å­˜å¯¹æ¯”ï¼š
# æ¯tokenï¼šMLA 1152 bytes + NSA index 132 bytes = 1284 bytes
# ä¼ ç»ŸMHAï¼š16384 bytes
# èŠ‚çœï¼š92%ï¼
```

#### 7. AscendTokenToKVPool - æ˜‡è…¾NPUç‰ˆæœ¬

**ä¸ºåä¸ºæ˜‡è…¾NPUä¼˜åŒ–**

```python
class AscendTokenToKVPool(MHATokenToKVPool):
    """æ˜‡è…¾NPUä¼˜åŒ–ç‰ˆæœ¬"""
    
    # å…³é”®ï¼šä½¿ç”¨å•ä¸€è¿ç»­å†…å­˜å—
    self.kv_buffer = torch.zeros(
        (2, layer_num, num_pages, page_size, head_num, head_dim)
    )
    #  â†‘    â†‘         â†‘          â†‘          â†‘         â†‘
    # K/V  å±‚æ•°      é¡µæ•°       é¡µå¤§å°      å¤´æ•°      å¤´ç»´åº¦
    
    self.k_buffer = self.kv_buffer[0]
    self.v_buffer = self.kv_buffer[1]
```

**ä½¿ç”¨NPUä¸“ç”¨ç®—å­**

```python
def set_kv_buffer(self, layer, loc, cache_k, cache_v, ...):
    torch_npu._npu_reshape_and_cache(
        key=cache_k,
        value=cache_v,
        key_cache=self.k_buffer[layer_id].view(-1, page_size, head_num, head_dim),
        value_cache=self.v_buffer[layer_id].view(-1, page_size, head_num, head_dim),
        slot_indices=loc,
    )

# NPUç®—å­ä¼˜åŠ¿ï¼š
# 1. èåˆreshapeå’Œå†™å…¥æ“ä½œ
# 2. ä½¿ç”¨NPUçš„å‘é‡å•å…ƒåŠ é€Ÿ
# 3. ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
# 4. å‡å°‘kernel launchå¼€é”€
```

#### 8. DoubleSparseTokenToKVPool - åŒç¨€ç–KVæ± 

```python
class DoubleSparseTokenToKVPool(KVCache):
    """ç”¨äºDouble Sparse Attentionï¼ˆåŒç¨€ç–æ³¨æ„åŠ›ï¼‰"""
    
    # åŒç¨€ç–æŒ‡ï¼š
    # 1. Tokenç¨€ç–ï¼šåªå…³æ³¨é‡è¦çš„token
    # 2. Channelç¨€ç–ï¼šåªå…³æ³¨é‡è¦çš„é€šé“ï¼ˆç»´åº¦ï¼‰
    
    self.k_buffer: List[torch.Tensor]  # æ ‡å‡†K
    self.v_buffer: List[torch.Tensor]  # æ ‡å‡†V
    self.label_buffer: List[torch.Tensor]  # é‡è¦é€šé“æ ‡ç­¾
    # å½¢çŠ¶ï¼š[size, head_num, heavy_channel_num]

# ä½¿ç”¨åœºæ™¯ï¼š
# å…ˆç”¨labelå¿«é€Ÿè¿‡æ»¤ï¼ˆåªè®¡ç®—16ç»´ï¼‰â†’ ç­›é€‰top tokens
# â†’ å†å¯¹top tokensè®¡ç®—å®Œæ•´attentionï¼ˆ128ç»´ï¼‰
```

---

## allocator.py - åˆ†é…å™¨å±‚

### BaseTokenToKVPoolAllocator - æŠ½è±¡åŸºç±»

#### åŒé˜Ÿåˆ—æœºåˆ¶

```python
class BaseTokenToKVPoolAllocator(abc.ABC):
    """åˆ†é…å™¨çš„æŠ½è±¡åŸºç±»"""
    
    # ç©ºé—²é¡µç®¡ç†ï¼ˆæ ¸å¿ƒæœºåˆ¶ï¼‰
    self.free_pages: torch.Tensor      # ç«‹å³å¯ç”¨çš„ç©ºé—²é¡µï¼ˆå·²æ’åºï¼‰
    self.release_pages: torch.Tensor   # å¾…åˆå¹¶çš„é‡Šæ”¾é¡µï¼ˆæœªæ’åºï¼‰
```

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªé˜Ÿåˆ—ï¼Ÿ**

```python
# åœºæ™¯ï¼šé¢‘ç¹åˆ†é…é‡Šæ”¾
for i in range(1000):
    alloc(10)
    free(10)
    # å¦‚æœæ¯æ¬¡freeéƒ½æ’åºï¼Œå¼€é”€å·¨å¤§ï¼

# è§£å†³æ–¹æ¡ˆï¼šå»¶è¿Ÿåˆå¹¶
free_pages = [1, 5, 7, 10, 15]     # å·²æ’åº
release_pages = [100, 50, 200]     # æœªæ’åº

# åªåœ¨éœ€è¦æ—¶æ‰åˆå¹¶æ’åº
if need_size > len(free_pages):
    merge_and_sort_free()
```

**ä¸ºä»€ä¹ˆéœ€è¦æ’åºï¼Ÿ**

```python
# æœªæ’åºï¼šfree_pages = [100, 5, 200, 6, 7]
indices = alloc(5)  # å¾—åˆ° [100, 5, 200, 6, 7]ï¼ˆä¸è¿ç»­ï¼‰

# æ’åºåï¼šfree_pages = [5, 6, 7, 100, 200]
indices = alloc(5)  # å¾—åˆ° [5, 6, 7, 100, 200]ï¼ˆå‰3ä¸ªè¿ç»­ï¼‰

# è¿ç»­å†…å­˜çš„å¥½å¤„ï¼š
# - Cacheå‹å¥½
# - å‡å°‘TLB miss
# - æ”¯æŒå‘é‡åŒ–æ“ä½œ
```

#### æ‰¹é‡é‡Šæ”¾ä¼˜åŒ–

```python
def free_group_begin(self):
    """å¼€å§‹æ‰¹é‡é‡Šæ”¾"""
    self.is_not_in_free_group = False
    self.free_group = []

def free_group_end(self):
    """ç»“æŸæ‰¹é‡é‡Šæ”¾ï¼Œä¸€æ¬¡æ€§å¤„ç†"""
    self.is_not_in_free_group = True
    if self.free_group:
        self.free(torch.cat(self.free_group))

# ä½¿ç”¨ï¼š
allocator.free_group_begin()
for req in finished_reqs:
    allocator.free(req.kv_indices)  # æš‚å­˜
allocator.free_group_end()  # ä¸€æ¬¡æ€§cat

# æ€§èƒ½ï¼š
# é€ä¸ªé‡Šæ”¾ï¼š100æ¬¡catæ“ä½œ
# æ‰¹é‡é‡Šæ”¾ï¼š1æ¬¡catæ“ä½œ
```

#### çŠ¶æ€ä¿å­˜æ¢å¤ï¼ˆç”¨äºæ¨æµ‹è§£ç ï¼‰

```python
# 1. å¤‡ä»½çŠ¶æ€
state = allocator.backup_state()

# 2. æ¨æµ‹æ€§åˆ†é…
indices = allocator.alloc(100)

# 3. éªŒè¯å¤±è´¥ï¼Œå›æ»š
if verification_failed:
    allocator.restore_state(state)
```

---

### TokenToKVPoolAllocator - Tokençº§åˆ†é…å™¨

```python
class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
    """æœ€ç®€å•çš„åˆ†é…å™¨ï¼štokençº§ç²’åº¦ï¼ˆpage_size=1ï¼‰"""
    
    def __init__(self, size, dtype, device, kvcache, need_sort):
        super().__init__(size, 1, dtype, device, kvcache, need_sort)
    
    def clear(self):
        # slot 0ä¿ç•™ç»™padding token
        self.free_pages = torch.arange(1, self.size + 1, 
                                       dtype=torch.int64, 
                                       device=self.device)
    
    def alloc(self, need_size: int):
        if self.need_sort and need_size > len(self.free_pages):
            self.merge_and_sort_free()
        
        if need_size > len(self.free_pages):
            return None
        
        select_index = self.free_pages[:need_size]
        self.free_pages = self.free_pages[need_size:]
        return select_index
    
    def free(self, free_index: torch.Tensor):
        if self.is_not_in_free_group:
            if self.need_sort:
                self.release_pages = torch.cat((self.release_pages, free_index))
            else:
                self.free_pages = torch.cat((self.free_pages, free_index))
        else:
            self.free_group.append(free_index)
```

---

### PagedTokenToKVPoolAllocator - é¡µå¯¹é½åˆ†é…å™¨

**æœ€å¤æ‚ä¹Ÿæ˜¯æœ€é«˜æ•ˆçš„åˆ†é…å™¨ï¼**

```python
class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
    """é¡µå¯¹é½åˆ†é…å™¨ï¼šä»¥pageä¸ºå•ä½ï¼ˆé€šå¸¸64 tokensï¼‰"""
    
    def __init__(self, size, page_size, dtype, device, kvcache, need_sort):
        super().__init__(size, page_size, dtype, device, kvcache, need_sort)
        self.num_pages = size // page_size
        self.seen_max_num_extend_tokens_next_power_of_2 = 1
```

#### åŸºç¡€é¡µåˆ†é…

```python
def alloc(self, need_size: int):
    """åˆ†é…need_sizeä¸ªtokenï¼ˆå¿…é¡»é¡µå¯¹é½ï¼‰"""
    num_pages = need_size // self.page_size
    
    # åˆ†é…é¡µ
    out_pages = self.free_pages[:num_pages]
    self.free_pages = self.free_pages[num_pages:]
    
    # å±•å¼€ä¸ºtokenç´¢å¼•
    out_indices = (
        out_pages[:, None] * self.page_size
        + torch.arange(self.page_size, device=self.device)
    ).reshape(-1)
    
    return out_indices

# ç¤ºä¾‹ï¼špage_size=64, åˆ†é…2é¡µ out_pages=[5,7]
# å±•å¼€ä¸ºï¼š[320,321,...,383, 448,449,...,511]
```

#### é«˜çº§ï¼šalloc_extendï¼ˆTriton Kernelä¼˜åŒ–ï¼‰

```python
def alloc_extend(
    self,
    prefix_lens: torch.Tensor,    # å·²æœ‰é•¿åº¦
    seq_lens: torch.Tensor,        # æ–°é•¿åº¦
    last_loc: torch.Tensor,        # æœ€åä¸€ä¸ªtokenä½ç½®
    extend_num_tokens: int,        # æ€»å…±è¦æ‰©å±•çš„tokenæ•°
):
    """ä½¿ç”¨Triton kernelé«˜æ•ˆå¹¶è¡Œåˆ†é…"""
    
    out_indices = torch.empty((extend_num_tokens,), 
                             dtype=torch.int64, 
                             device=self.device)
    
    # è°ƒç”¨Triton kernel
    bs = len(prefix_lens)
    alloc_extend_kernel[(bs,)](
        prefix_lens,
        seq_lens,
        last_loc,
        self.free_pages,
        out_indices,
        next_power_of_2(bs),
        self.page_size,
        self.seen_max_num_extend_tokens_next_power_of_2,
    )
    
    return out_indices
```

#### Triton Kernelä¸‰é˜¶æ®µå¡«å……

```python
@triton.jit
def alloc_extend_kernel(...):
    """ä¸‰é˜¶æ®µå¡«å……ç­–ç•¥"""
    
    # Part 1: å¡«å……æ—§çš„éƒ¨åˆ†é¡µ
    # å¦‚æœprefixç»“æŸåœ¨é¡µä¸­é—´ï¼Œå…ˆæŠŠè¿™ä¸€é¡µå¡«æ»¡
    num_part1 = min(seq_len, next_page_boundary) - pre_len
    
    # Part 2: å¡«å……æ–°çš„å®Œæ•´é¡µ
    # åˆ†é…æ•´é¡µï¼Œè¿ç»­å¡«å……
    num_part2 = (seq_len // page_size * page_size) - ...
    
    # Part 3: å¡«å……æ–°çš„éƒ¨åˆ†é¡µ
    # æœ€åå¯èƒ½æœ‰ä¸ªä¸å®Œæ•´çš„é¡µ
    num_part3 = seq_len % page_size

# ç¤ºä¾‹ï¼špage_size=4, prefix_len=6, seq_len=13
# æ—§æ•°æ®: [0,1,2,3][4,5,_,_]
# æ‰©å±•å: [0,1,2,3][4,5,6,7][8,9,10,11][12,_,_,_]
#                   â””Part1â”˜ â””â”€Part2â”€â”€â”˜ â””Part3â”˜
```

#### alloc_decodeï¼ˆæ¯æ¬¡åªåˆ†é…1ä¸ªtokenï¼‰

```python
def alloc_decode(self, seq_lens, last_loc):
    """Decodeé˜¶æ®µï¼šæ¯ä¸ªè¯·æ±‚åªéœ€1ä¸ªtoken"""
    
    bs = len(seq_lens)
    out_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)
    
    alloc_decode_kernel[(bs,)](
        seq_lens,
        last_loc,
        self.free_pages,
        out_indices,
        next_power_of_2(bs),
        self.page_size,
    )
    
    return out_indices

# åŸç†ï¼š
# - å¦‚æœå½“å‰é¡µæœªæ»¡ï¼šlast_loc + 1
# - å¦‚æœå½“å‰é¡µå·²æ»¡ï¼šåˆ†é…æ–°é¡µçš„ç¬¬ä¸€ä¸ªä½ç½®
```

---

### SWATokenToKVPoolAllocator - SWAæ··åˆåˆ†é…å™¨

```python
class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
    """ä¸ºLlama4æ··åˆæ¨¡å‹ç®¡ç†åŒæ± åˆ†é…"""
    
    def __init__(self, size, size_swa, dtype, device, kvcache, need_sort):
        super().__init__(size, 1, dtype, device, kvcache, need_sort)
        
        # åˆ›å»ºä¸¤ä¸ªå­åˆ†é…å™¨
        self.full_attn_allocator = TokenToKVPoolAllocator(size, ...)
        self.swa_attn_allocator = TokenToKVPoolAllocator(size_swa, ...)
        
        # ç»´æŠ¤æ˜ å°„
        self.full_to_swa_index_mapping = torch.empty(
            size + size_swa + 1, dtype=torch.int64, device=device
        )
    
    def alloc(self, need_size: int):
        """åŒæ—¶ä»ä¸¤ä¸ªæ± ä¸­åˆ†é…"""
        # æ£€æŸ¥å®¹é‡
        if need_size > self.full_attn_allocator.available_size():
            return None
        if need_size > self.swa_attn_allocator.available_size():
            return None
        
        # åŒæ—¶åˆ†é…
        alloc_full_indices = self.full_attn_allocator.alloc(need_size)
        alloc_swa_indices = self.swa_attn_allocator.alloc(need_size)
        
        # å»ºç«‹æ˜ å°„
        self.full_to_swa_index_mapping[alloc_full_indices] = alloc_swa_indices
        
        # å¯¹å¤–è¿”å›å…¨æ³¨æ„åŠ›ç´¢å¼•
        return alloc_full_indices
    
    def free(self, free_index: torch.Tensor):
        """åŒæ—¶é‡Šæ”¾ä¸¤ä¸ªæ± """
        self.full_attn_allocator.free(free_index)
        
        # æŸ¥æ‰¾å¹¶é‡Šæ”¾å¯¹åº”çš„SWAç´¢å¼•
        swa_indices = self.full_to_swa_index_mapping[free_index]
        swa_indices = swa_indices[swa_indices > 0]
        self.swa_attn_allocator.free(swa_indices)
        
        # æ¸…ç†æ˜ å°„
        self.full_to_swa_index_mapping[free_index] = 0
```

---

## memory_pool_host.py - Hostå†…å­˜å±‚

### HostKVCache - HiCache L2å±‚

**HiCacheæ¶æ„å›é¡¾**

```
L1 (GPU Memory):  æœ€å¿«ï¼Œå®¹é‡å°ï¼ˆå¦‚32GBï¼‰
    â†“
L2 (Host Memory): è¾ƒå¿«ï¼Œå®¹é‡å¤§ï¼ˆå¦‚256GBï¼‰â† memory_pool_host.py
    â†“
L3 (Distributed Storage): è¾ƒæ…¢ï¼Œå®¹é‡å·¨å¤§ï¼ˆTBçº§ï¼‰
```

#### æ ¸å¿ƒè®¾è®¡

```python
class HostKVCache(abc.ABC):
    """CPUå†…å­˜ä¸­çš„KV cacheæ± ï¼ˆHiCache L2å±‚ï¼‰"""
    
    def __init__(
        self,
        device_pool: KVCache,           # å¯¹åº”çš„GPUæ± 
        host_to_device_ratio: float,    # Host/GPUå†…å­˜æ¯”ä¾‹
        host_size: int,                 # Hostå†…å­˜å¤§å°(GB)
        page_size: int,
        layout: str,                    # å†…å­˜å¸ƒå±€
        pin_memory: bool,               # æ˜¯å¦ä½¿ç”¨pinned memory
        device: str = "cpu",
    ):
        # è®¡ç®—Hostå†…å­˜å®¹é‡
        if host_size > 0:
            self.size = int(host_size * 1e9 // self.size_per_token)
        else:
            self.size = int(device_pool.size * host_to_device_ratio)
        
        # é¡µå¯¹é½
        self.size = self.size - (self.size % self.page_size)
        self.page_num = self.size // self.page_size
        
        # æ£€æŸ¥Hostå†…å­˜æ˜¯å¦è¶³å¤Ÿ
        host_mem = psutil.virtual_memory()
        requested_bytes = self.size * self.size_per_token
        available_bytes = host_mem.available - 10 * (1024**3)  # ä¿ç•™10GB
        
        if requested_bytes > available_bytes:
            raise ValueError("Not enough host memory!")
        
        # åˆ†é…å†…å­˜
        self.kv_buffer = self.init_kv_buffer()
        
        # çº¿ç¨‹å®‰å…¨
        self.lock = threading.RLock()
```

#### ä¸‰ç§å†…å­˜å¸ƒå±€

```python
# 1. layer_first (ä¼ ç»Ÿå¸ƒå±€)
dims = (2, layer_num, size, head_num, head_dim)
# [K/V, å±‚, Token, å¤´, ç»´åº¦]
# ä¼˜ç‚¹ï¼šæŒ‰å±‚è®¿é—®è¿ç»­
# ç¼ºç‚¹ï¼šè·¨å±‚è®¿é—®ä¸è¿ç»­

# 2. page_first (HiCacheä¼˜åŒ–)
dims = (2, size, layer_num, head_num, head_dim)
# [K/V, Token, å±‚, å¤´, ç»´åº¦]
# ä¼˜ç‚¹ï¼šæŒ‰tokenè®¿é—®è¿ç»­ï¼ˆé€‚åˆé¡µç²’åº¦ä¼ è¾“ï¼‰
# ç¼ºç‚¹ï¼šæŒ‰å±‚è®¿é—®ä¸è¿ç»­

# 3. page_first_direct (è¿›ä¸€æ­¥ä¼˜åŒ–)
dims = (2, page_num, layer_num, page_size, head_num, head_dim)
# [K/V, é¡µ, å±‚, é¡µå†…Token, å¤´, ç»´åº¦]
# ä¼˜ç‚¹ï¼šé¡µç²’åº¦ç»„ç»‡ï¼Œé›¶æ‹·è´å‹å¥½
```

**å¸ƒå±€é€‰æ‹©å»ºè®®**

```python
# layer_first: 
# - é€‚åˆï¼šdirect IO backend
# - åœºæ™¯ï¼šæŒ‰å±‚åŠ è½½

# page_first: 
# - é€‚åˆï¼škernel IO backendï¼ˆé›¶æ‹·è´ï¼‰
# - åœºæ™¯ï¼šé¡µç²’åº¦ä¼ è¾“
# - æ³¨æ„ï¼šä¸å…¼å®¹direct backend

# page_first_direct:
# - é€‚åˆï¼šdirect IO backend + FA3
# - åœºæ™¯ï¼šé¡µç²’åº¦ + ç›´æ¥IO
# - ä¼˜ç‚¹ï¼šåŒæ—¶æ”¯æŒé›¶æ‹·è´å’ŒFA3
```

#### ä¸¤ç§IOåç«¯

```python
# 1. kernel backend (é›¶æ‹·è´ï¼Œæ¨è)
# ä½¿ç”¨sgl_kernelçš„kvcacheioæ¨¡å—
# ç‰¹ç‚¹ï¼šGPUç›´æ¥è®¿é—®CPU pinned memory
# æ€§èƒ½ï¼šæœ€å¿«

if io_backend == "kernel":
    if layout == "layer_first":
        transfer_kv_per_layer(
            src_k, dst_k, src_v, dst_v,
            src_indices, dst_indices, item_size
        )
    elif layout == "page_first":
        transfer_kv_per_layer_pf_lf(...)

# 2. direct backend (é€šç”¨)
# ä½¿ç”¨PyTorchçš„.to()æ“ä½œ
# ç‰¹ç‚¹ï¼šå…¼å®¹æ€§å¥½ï¼Œä½†æ€§èƒ½ç¨æ…¢
# æ€§èƒ½ï¼šæ¯”kernelæ…¢10-20%

elif io_backend == "direct":
    transfer_kv_direct(
        src_layers, dst_layers,
        src_indices, dst_indices, page_size
    )
```

#### æ ¸å¿ƒæ–¹æ³•

```python
@synchronized
def alloc(self, need_size: int) -> Optional[torch.Tensor]:
    """çº¿ç¨‹å®‰å…¨çš„åˆ†é…"""
    assert need_size % self.page_size == 0
    if need_size > self.available_size():
        return None
    
    select_index = self.free_slots[:need_size]
    self.free_slots = self.free_slots[need_size:]
    return select_index

@synchronized
def free(self, indices: torch.Tensor) -> int:
    """çº¿ç¨‹å®‰å…¨çš„é‡Šæ”¾"""
    self.free_slots = torch.cat([self.free_slots, indices])
    return len(indices)

def load_to_device_per_layer(
    self, device_pool, host_indices, device_indices, layer_id, io_backend
):
    """ä»HoståŠ è½½KV cacheåˆ°GPUï¼ˆæŒ‰å±‚ï¼‰"""
    # æ ¹æ®layoutå’Œio_backendé€‰æ‹©åˆé€‚çš„ä¼ è¾“å‡½æ•°
    if io_backend == "kernel":
        if self.layout == "layer_first":
            transfer_kv_per_layer(...)
        elif self.layout == "page_first":
            transfer_kv_per_layer_pf_lf(...)
    elif io_backend == "direct":
        transfer_kv_direct(...)

def backup_from_device_all_layer(
    self, device_pool, host_indices, device_indices, io_backend
):
    """ä»GPUå¤‡ä»½KV cacheåˆ°Hostï¼ˆæ‰€æœ‰å±‚ï¼‰"""
    # ä¸€æ¬¡æ€§ä¼ è¾“æ‰€æœ‰å±‚ï¼Œå‡å°‘kernel launchå¼€é”€
    if io_backend == "kernel":
        transfer_kv_all_layer(
            src_k_layers, dst_k_layers,
            src_v_layers, dst_v_layers,
            src_indices, dst_indices,
            item_size, num_layers
        )
```

#### é›¶æ‹·è´æ”¯æŒï¼ˆä¸L3é›†æˆï¼‰

```python
def get_page_buffer_meta(self, indices):
    """
    è¿”å›é¡µçš„å†…å­˜åœ°å€å’Œå¤§å°ï¼ˆç”¨äºé›¶æ‹·è´ä¼ è¾“åˆ°L3ï¼‰
    """
    ptr_list = []
    kv_buffer_data_ptr = self.kv_buffer.data_ptr()
    
    if self.layout == "layer_first":
        for index in range(0, len(indices), self.page_size):
            for layer_id in range(self.layer_num):
                # è®¡ç®—Kå’ŒVçš„å†…å­˜åœ°å€
                k_ptr = kv_buffer_data_ptr + ...
                v_ptr = k_ptr + v_offset
                ptr_list.append(k_ptr)
                ptr_list.append(v_ptr)
    
    elif self.layout in ["page_first", "page_first_direct"]:
        # page_firstå¸ƒå±€ï¼šé¡µè¿ç»­
        for index in range(0, len(indices), self.page_size):
            k_ptr = kv_buffer_data_ptr + ...
            v_ptr = k_ptr + v_offset
            ptr_list.append(k_ptr)
            ptr_list.append(v_ptr)
    
    return ptr_list, element_size_list

# ç”¨é€”ï¼šL3å­˜å‚¨ï¼ˆå¦‚Mooncakeï¼‰å¯ä»¥ç›´æ¥RDMAè¯»å†™è¿™äº›åœ°å€
# å®ç°çœŸæ­£çš„é›¶æ‹·è´ä¼ è¾“
```

---

### MHATokenToKVPoolHost

```python
class MHATokenToKVPoolHost(HostKVCache):
    """MHAæ¨¡å‹çš„Host KVæ± """
    
    def get_size_per_token(self):
        """è®¡ç®—æ¯ä¸ªtokençš„å­˜å‚¨å¤§å°"""
        return (
            self.head_dim 
            * self.head_num 
            * self.layer_num 
            * self.dtype.itemsize 
            * 2  # Kå’ŒV
        )
    
    def init_kv_buffer(self):
        """æ ¹æ®layoutåˆå§‹åŒ–å†…å­˜"""
        if self.layout == "layer_first":
            dims = (2, self.layer_num, self.size, 
                   self.head_num, self.head_dim)
        elif self.layout == "page_first":
            dims = (2, self.size, self.layer_num, 
                   self.head_num, self.head_dim)
        elif self.layout == "page_first_direct":
            dims = (2, self.page_num, self.layer_num, 
                   self.page_size, self.head_num, self.head_dim)
        
        return torch.empty(
            dims, 
            dtype=self.dtype,
            device=self.device,
            pin_memory=self.pin_memory,  # å…³é”®ï¼šå¯ç”¨pinned memory
        )
    
    @property
    def k_buffer(self):
        return self.kv_buffer[0]
    
    @property
    def v_buffer(self):
        return self.kv_buffer[1]
```

---

### MLATokenToKVPoolHost

```python
class MLATokenToKVPoolHost(HostKVCache):
    """MLAæ¨¡å‹çš„Host KVæ± """
    
    def get_size_per_token(self):
        """MLAå‹ç¼©åçš„å¤§å°"""
        return (
            (self.kv_lora_rank + self.qk_rope_head_dim)
            * 1  # åªæœ‰1ä¸ªå¤´
            * self.dtype.itemsize
            * self.layer_num
        )
    
    def init_kv_buffer(self):
        """MLAåªéœ€è¦ä¸€ä¸ªKV buffer"""
        if self.layout == "layer_first":
            dims = (self.layer_num, self.size, 1,
                   self.kv_lora_rank + self.qk_rope_head_dim)
        elif self.layout == "page_first":
            dims = (self.size, self.layer_num, 1,
                   self.kv_lora_rank + self.qk_rope_head_dim)
        elif self.layout == "page_first_direct":
            dims = (self.page_num, self.layer_num, 
                   self.page_size, 1,
                   self.kv_lora_rank + self.qk_rope_head_dim)
        
        return torch.empty(dims, dtype=self.dtype, device=self.device,
                          pin_memory=self.pin_memory)
```

---

## è®¾è®¡æ¨¡å¼æ€»ç»“

### 1. åˆ†å±‚æŠ½è±¡æ¨¡å¼

```
åº”ç”¨å±‚ï¼šè¯·æ±‚ç®¡ç†
    â†“
ReqToTokenPoolï¼šä¸šåŠ¡å±‚ï¼ˆè¯·æ±‚â†’Tokenä½ç½®ï¼‰
    â†“
Allocatorï¼šç®¡ç†å±‚ï¼ˆåˆ†é…ç­–ç•¥ï¼‰
    â†“
KVCacheï¼šå­˜å‚¨å±‚ï¼ˆç‰©ç†å†…å­˜ï¼‰
    â†“
HostKVCacheï¼šæ‰©å±•å±‚ï¼ˆHostå†…å­˜ï¼‰
```

**ä¼˜ç‚¹**ï¼š
- èŒè´£æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤
- å±‚æ¬¡è§£è€¦ï¼Œæ˜“äºæ‰©å±•
- æ”¯æŒä¸åŒæ¶æ„ï¼ˆMHA/MLA/Mamba/Hybridï¼‰

### 2. æ¨¡æ¿æ–¹æ³•æ¨¡å¼

```python
# æŠ½è±¡åŸºç±»å®šä¹‰æ¥å£
class KVCache(abc.ABC):
    @abc.abstractmethod
    def get_key_buffer(self, layer_id): ...
    
    @abc.abstractmethod
    def set_kv_buffer(self, layer, loc, k, v): ...

# å…·ä½“å­ç±»å®ç°ç»†èŠ‚
class MHATokenToKVPool(KVCache):
    def get_key_buffer(self, layer_id):
        return self.k_buffer[layer_id]
    
    def set_kv_buffer(self, layer, loc, k, v):
        self.k_buffer[layer_id][loc] = k
        self.v_buffer[layer_id][loc] = v

class MLATokenToKVPool(KVCache):
    def get_key_buffer(self, layer_id):
        return self.kv_buffer[layer_id]  # è¿”å›å‹ç¼©çš„KV
    
    def set_kv_buffer(self, layer, loc, k, v):
        # ä½¿ç”¨Triton kernelæ‹¼æ¥
        set_mla_kv_buffer_triton(self.kv_buffer[layer_id], loc, k, v)
```

### 3. ç­–ç•¥æ¨¡å¼

```python
# ä¸åŒçš„åˆ†é…ç­–ç•¥
class TokenToKVPoolAllocator:
    """Tokençº§åˆ†é…ç­–ç•¥"""
    page_size = 1

class PagedTokenToKVPoolAllocator:
    """é¡µå¯¹é½åˆ†é…ç­–ç•¥"""
    page_size = 64
    
    def alloc_extend(self, ...):
        # ä½¿ç”¨Triton kernelä¼˜åŒ–

class SWATokenToKVPoolAllocator:
    """åŒæ± åˆ†é…ç­–ç•¥"""
    def alloc(self, need_size):
        # åŒæ—¶ä»ä¸¤ä¸ªæ± åˆ†é…
```

### 4. é€‚é…å™¨æ¨¡å¼

```python
# HybridLinearKVPoolé€‚é…ä¸åŒç±»å‹çš„å±‚
class HybridLinearKVPool(KVCache):
    def __init__(self, full_attention_layer_ids, ...):
        self.full_kv_pool = MHATokenToKVPool(...)
        # Mambaå±‚ä¸éœ€è¦KV cache
    
    def get_key_buffer(self, layer_id):
        # å°†å…¨å±€å±‚IDè½¬æ¢ä¸ºæ± å†…å±€éƒ¨ID
        local_id = self._transfer_full_attention_id(layer_id)
        return self.full_kv_pool.get_key_buffer(local_id)
```

### 5. å·¥å‚æ¨¡å¼

```python
# æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºç›¸åº”çš„KVæ± 
if model_type == "mha":
    kv_pool = MHATokenToKVPool(...)
elif model_type == "mla":
    kv_pool = MLATokenToKVPool(...)
elif model_type == "hybrid_gdn":
    kv_pool = HybridLinearKVPool(...)
elif model_type == "swa":
    kv_pool = SWAKVPool(...)
```

### 6. è§‚å¯Ÿè€…æ¨¡å¼ï¼ˆHiCacheï¼‰

```python
# Layer Transfer Counteré€šçŸ¥æœºåˆ¶
class KVCache:
    def register_layer_transfer_counter(self, counter):
        self.layer_transfer_counter = counter
    
    def get_key_buffer(self, layer_id):
        if self.layer_transfer_counter is not None:
            # ç­‰å¾…è¯¥å±‚æ•°æ®åŠ è½½å®Œæˆ
            self.layer_transfer_counter.wait_until(layer_id)
        return self._get_key_buffer(layer_id)
```

---

## æ€§èƒ½ä¼˜åŒ–æ€»ç»“

### 1. å†…å­˜ä¼˜åŒ–

#### a. å»¶è¿Ÿæ’åºï¼ˆAllocatorï¼‰
```python
# é—®é¢˜ï¼šé¢‘ç¹freeå¯¼è‡´å¤§é‡æ’åºå¼€é”€
# æ–¹æ¡ˆï¼šä½¿ç”¨release_pagesæš‚å­˜ï¼Œå»¶è¿Ÿåˆ°éœ€è¦æ—¶æ‰æ’åº

# ä¼ ç»Ÿï¼š
for i in range(1000):
    free(indices)
    # æ¯æ¬¡éƒ½æ’åºï¼Œ1000æ¬¡sortï¼

# ä¼˜åŒ–ï¼š
for i in range(1000):
    free(indices)  # æ·»åŠ åˆ°release_pages
# åªåœ¨allocæ—¶æ‰merge_and_sort_free()ï¼Œ1æ¬¡sortï¼
```

#### b. æ‰¹é‡æ“ä½œï¼ˆAllocatorï¼‰
```python
# é—®é¢˜ï¼šé€ä¸ªé‡Šæ”¾å¯¼è‡´å¤§é‡catæ“ä½œ
# æ–¹æ¡ˆï¼šæ‰¹é‡æ”¶é›†åä¸€æ¬¡æ€§cat

# ä¼ ç»Ÿï¼š
for req in 100_requests:
    allocator.free(req.indices)  # 100æ¬¡cat

# ä¼˜åŒ–ï¼š
allocator.free_group_begin()
for req in 100_requests:
    allocator.free(req.indices)  # åªappend
allocator.free_group_end()  # 1æ¬¡cat
```

#### c. é›¶æ‹·è´ä¼ è¾“ï¼ˆHostKVCacheï¼‰
```python
# é—®é¢˜ï¼šCPU-GPUä¼ è¾“å¼€é”€å¤§
# æ–¹æ¡ˆï¼šä½¿ç”¨pinned memory + kernel backend

# ä¼ ç»Ÿæ–¹å¼ï¼š
host_data = kv_cache.cpu()  # å¤åˆ¶åˆ°pageable memory
gpu_data = host_data.to('cuda')  # å†å¤åˆ¶åˆ°GPU

# é›¶æ‹·è´ï¼š
host_data = torch.empty(..., pin_memory=True)  # pinned memory
transfer_kv_per_layer(host_data, gpu_data, ...)  # GPUç›´æ¥DMAè®¿é—®
```

### 2. è®¡ç®—ä¼˜åŒ–

#### a. Triton Kernelå¹¶è¡Œåˆ†é…
```python
# é—®é¢˜ï¼šPythonå¾ªç¯åˆ†é…æ…¢
# æ–¹æ¡ˆï¼šä½¿ç”¨Triton kernelå¹¶è¡Œå¤„ç†

# ä¼ ç»Ÿï¼š
for i in range(batch_size):
    indices[i] = allocate_for_request(i)  # Pythonå¾ªç¯

# ä¼˜åŒ–ï¼š
alloc_extend_kernel[(batch_size,)](...)  # GPUå¹¶è¡Œ
```

#### b. åŒæµé‡å ï¼ˆMHATokenToKVPoolï¼‰
```python
# é—®é¢˜ï¼šKå’ŒVå†™å…¥ä¸²è¡Œ
# æ–¹æ¡ˆï¼šä½¿ç”¨ä¸¤ä¸ªCUDA streamå¹¶è¡Œå†™å…¥

# ä¼ ç»Ÿï¼š
k_buffer[loc] = k  # 100us
v_buffer[loc] = v  # 100us
# æ€»è®¡ï¼š200us

# ä¼˜åŒ–ï¼š
with main_stream:
    k_buffer[loc] = k
with alt_stream:  # å¹¶è¡Œæ‰§è¡Œ
    v_buffer[loc] = v
# æ€»è®¡ï¼š~110us
```

#### c. MLAå‹ç¼©ï¼ˆMLATokenToKVPoolï¼‰
```python
# é—®é¢˜ï¼šKV cacheå ç”¨æ˜¾å­˜å¤ªå¤§
# æ–¹æ¡ˆï¼šä½ç§©å‹ç¼©

# MHAï¼š32å¤´Ã—128ç»´Ã—2(K/V) = 8192ç»´/token
# MLAï¼š512(lora)+64(rope) = 576ç»´/token
# å‹ç¼©æ¯”ï¼š93%èŠ‚çœ
```

### 3. æ¶æ„ä¼˜åŒ–

#### a. Hybridæ¨¡å‹æ··åˆç¼“å­˜
```python
# HybridLinearKVPoolï¼šåªä¸ºå…¨æ³¨æ„åŠ›å±‚åˆ†é…KV cache
# Mambaå±‚ä½¿ç”¨MambaPoolï¼ˆçŠ¶æ€ç¼“å­˜ï¼‰
# èŠ‚çœï¼š~50%ï¼ˆQwen3Nextï¼‰

# SWAKVPoolï¼šSWAå±‚ç”¨å°ç¼“å­˜ï¼Œå…¨æ³¨æ„åŠ›å±‚ç”¨å¤§ç¼“å­˜
# èŠ‚çœï¼š~73%ï¼ˆLlama4ï¼‰
```

#### b. NSAç¨€ç–æ£€ç´¢
```python
# ä¸¤çº§æ£€ç´¢ï¼š
# 1. index_kå¿«é€Ÿç­›é€‰ï¼ˆ128ç»´FP8ï¼‰
# 2. å®Œæ•´attentionï¼ˆ576ç»´ï¼‰
# åªå¯¹top-k tokenè®¡ç®—å®Œæ•´attention
```

#### c. åˆ†é¡µç®¡ç†
```python
# page_size=64çš„ä¼˜åŠ¿ï¼š
# 1. ä¸HiCacheé¡µç²’åº¦ä¸€è‡´
# 2. æ”¯æŒé›¶æ‹·è´ä¼ è¾“
# 3. å‡å°‘å†…å­˜ç¢ç‰‡
# 4. ç®€åŒ–å¹¶è¡Œå¤„ç†
```

### 4. I/Oä¼˜åŒ–

#### a. åˆ†å±‚åŠ è½½ï¼ˆHostKVCacheï¼‰
```python
# æŒ‰å±‚å¼‚æ­¥åŠ è½½ï¼Œè¾¹åŠ è½½è¾¹è®¡ç®—
for layer in range(num_layers):
    load_to_device_per_layer(layer)  # å¼‚æ­¥å¯åŠ¨
    wait_until(layer)  # è®¡ç®—å‰ç­‰å¾…
    forward(layer)  # è®¡ç®—è¯¥å±‚
    # ä¸‹ä¸€å±‚å¯èƒ½å·²åœ¨åå°åŠ è½½å®Œæˆ
```

#### b. ä¸‰çº§ç¼“å­˜å±‚æ¬¡ï¼ˆHiCacheï¼‰
```
L1 (GPU):  32GB,  å»¶è¿Ÿ~100ns,   å‘½ä¸­ç‡80%
L2 (Host): 256GB, å»¶è¿Ÿ~10us,    å‘½ä¸­ç‡15%
L3 (åˆ†å¸ƒå¼): TBçº§,  å»¶è¿Ÿ~1ms,     å‘½ä¸­ç‡5%

æœ‰æ•ˆå»¶è¿Ÿ = 80%Ã—100ns + 15%Ã—10us + 5%Ã—1ms
        â‰ˆ 0.08us + 1.5us + 50us = 51.58us
vs å…¨éƒ¨L3ï¼š1000us
åŠ é€Ÿï¼š19.4x
```

### 5. å¹¶å‘ä¼˜åŒ–

#### a. çº¿ç¨‹å®‰å…¨ï¼ˆHostKVCacheï¼‰
```python
@synchronized  # ä½¿ç”¨RLock
def alloc(self, need_size):
    # å¤šçº¿ç¨‹å®‰å…¨çš„åˆ†é…
    ...

# æ”¯æŒå¹¶å‘ï¼š
# - ä¸»çº¿ç¨‹ï¼šforwardè®¡ç®—
# - åå°çº¿ç¨‹ï¼šL2/L3æ•°æ®ä¼ è¾“
```

#### b. å¼‚æ­¥ä¼ è¾“
```python
# PyTorchçš„non_blockingä¼ è¾“
host_data.to('cuda', non_blocking=True)
# CPUå’ŒGPUæ“ä½œå¯ä»¥overlap
```

---

## æ€»ç»“

### æ ¸å¿ƒç»„ä»¶å¯¹æ¯”

| ç»„ä»¶ | ä½ç½® | ä½œç”¨ | é€‚ç”¨æ¨¡å‹ |
|------|------|------|----------|
| **ReqToTokenPool** | memory_pool.py | è¯·æ±‚â†’Tokenæ˜ å°„ | æ‰€æœ‰æ¨¡å‹ |
| **MambaPool** | memory_pool.py | MambaçŠ¶æ€ç¼“å­˜ | Hybrid GDN |
| **HybridReqToTokenPool** | memory_pool.py | Token+Mambaæ··åˆ | Qwen3Next, FalconH1 |
| **MHATokenToKVPool** | memory_pool.py | æ ‡å‡†KV cache | Llama, GPTç­‰ |
| **MLATokenToKVPool** | memory_pool.py | å‹ç¼©KV cache | DeepSeek-V2/V3 |
| **NSATokenToKVPool** | memory_pool.py | ç¨€ç–ç´¢å¼•ä¼˜åŒ– | DeepSeek-V3 |
| **SWAKVPool** | memory_pool.py | SWAæ··åˆç¼“å­˜ | Llama4 |
| **HybridLinearKVPool** | memory_pool.py | æ··åˆå±‚ç¼“å­˜ | Qwen3Next |
| **AscendTokenToKVPool** | memory_pool.py | NPUä¼˜åŒ– | æ˜‡è…¾NPU |
| **TokenToKVPoolAllocator** | allocator.py | Tokençº§åˆ†é… | åŸºç¡€åœºæ™¯ |
| **PagedTokenToKVPoolAllocator** | allocator.py | é¡µå¯¹é½åˆ†é… | HiCacheåœºæ™¯ |
| **SWATokenToKVPoolAllocator** | allocator.py | åŒæ± åˆ†é… | Llama4 |
| **HostKVCache** | memory_pool_host.py | Hostå†…å­˜æ±  | HiCache L2 |

### å†…å­˜èŠ‚çœå¯¹æ¯”

| æŠ€æœ¯ | èŠ‚çœæ¯”ä¾‹ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| **MLAå‹ç¼©** | ~93% | DeepSeekæ¨¡å‹ |
| **NSAç¨€ç–** | ~92% | é•¿ä¸Šä¸‹æ–‡ç¨€ç–æ³¨æ„åŠ› |
| **Llama4 SWA** | ~73% | è¶…é•¿ä¸Šä¸‹æ–‡ |
| **Hybrid GDN** | ~50% | æ··åˆæ¶æ„æ¨¡å‹ |
| **HiCache L2** | å®é™…å®¹é‡8x | å¤šè½®å¯¹è¯ |

### æ€§èƒ½æå‡å¯¹æ¯”

| ä¼˜åŒ– | æå‡ | å…³é”®æŠ€æœ¯ |
|------|------|----------|
| **é›¶æ‹·è´ä¼ è¾“** | 2-3x | pinned memory + kernel backend |
| **åŒæµå†™å…¥** | 1.8x | CUDA streams overlap |
| **æ‰¹é‡æ“ä½œ** | 10-100x | å‡å°‘Pythonå¼€é”€ |
| **Triton kernel** | 5-10x | GPUå¹¶è¡Œ |
| **HiCacheå±‚æ¬¡** | 19x | ä¸‰çº§ç¼“å­˜ |

---

**æ–‡æ¡£ç»“æŸ**

*è¿™ä»½ç¬”è®°æ¶µç›–äº†SGLangå†…å­˜æ± ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å’Œè®¾è®¡æ¨¡å¼ã€‚å»ºè®®ç»“åˆä»£ç å’Œæœ¬æ–‡æ¡£ä¸€èµ·å­¦ä¹ ï¼Œä»¥æ›´å¥½åœ°ç†è§£å®ç°ç»†èŠ‚ã€‚*

