# ğŸ“ VILA å®Œæ•´å®ç°è§£æ

> å­¦ä¹ ç¬”è®°ï¼šç†è§£ SGLang ä¸­ VLM çš„å®ç°æ¨¡å¼

## ğŸ“š ç›®å½•ç»“æ„

```
VILA å®ç°åˆ†ä¸º 3 ä¸ªæ ¸å¿ƒæ–‡ä»¶ï¼š
â”œâ”€â”€ models/vila.py (307è¡Œ) - æ¨¡å‹ä¸»ä½“
â”œâ”€â”€ multimodal/processors/vila.py (70è¡Œ) - å¤šæ¨¡æ€æ•°æ®å¤„ç†
â””â”€â”€ test/srt/test_vision_openai_server_a.py - é›†æˆæµ‹è¯•
```

---

## ğŸ—ï¸ ä¸€ã€æ¶æ„è®¾è®¡ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰

### æ•´ä½“æ¶æ„å›¾

```
ç”¨æˆ·è¾“å…¥ (å›¾åƒ + æ–‡æœ¬)
    â†“
VILAMultimodalProcessor (å¤„ç†å™¨)
    â†“ é¢„å¤„ç†
VILAForConditionalGeneration (ä¸»æ¨¡å‹)
    â”œâ”€â†’ SiglipVisionModel (è§†è§‰ç¼–ç å™¨)
    â”‚       â†“ [batch, 3, 384, 384] â†’ [batch, 729, 1152]
    â”œâ”€â†’ MultimodalProjector (æŠ•å½±å±‚)
    â”‚       â†“ 3x3 ä¸‹é‡‡æ · + MLP
    â”‚       â†“ [batch, 729, 1152] â†’ [batch, 81, 1536]
    â””â”€â†’ Qwen2ForCausalLM (è¯­è¨€æ¨¡å‹)
            â†“ èåˆå›¾åƒ+æ–‡æœ¬
            â†“ [batch, seq_len, 1536] â†’ [batch, seq_len, vocab_size]
        LogitsProcessor (è¾“å‡º)
```

---

## ğŸ“ äºŒã€æ ¸å¿ƒä»£ç è¯¦è§£

### 1ï¸âƒ£ **é…ç½®ç±» (VILAConfig)** - ç¬¬34-86è¡Œ

```python
class VILAConfig(PretrainedConfig):
    model_type: str = "vila"
    
    # å…³é”®é…ç½®é¡¹ï¼š
    text_config: Qwen2Config          # LLM é…ç½®
    vision_config: SiglipVisionConfig # ViT é…ç½®
    
    # å¤šæ¨¡æ€é…ç½®
    hidden_size: int = 1536           # LLM éšè—å±‚å¤§å°
    mm_hidden_size: int = 1152        # Vision è¾“å‡ºå¤§å°
    image_token_id: int = 151649      # å›¾åƒå ä½ç¬¦ token ID
    
    # æŠ•å½±å±‚é…ç½®
    mm_projector_type: str = "mlp_downsample_3x3_fix"  # æŠ•å½±å±‚ç±»å‹
    mm_vision_select_layer: int = -2  # é€‰æ‹© ViT çš„ç¬¬å‡ å±‚è¾“å‡º
    mm_vision_select_feature: str = "cls_patch"  # é€‰æ‹©ç‰¹å¾ç±»å‹
```

**å…³é”®ç‚¹ï¼š**
- `text_config` å’Œ `vision_config` åˆ†åˆ«é…ç½® LLM å’Œ ViT
- `mm_hidden_size` (1152) â†’ `hidden_size` (1536) éœ€è¦æŠ•å½±å±‚å¯¹é½

---

### 2ï¸âƒ£ **æŠ•å½±å±‚ (MultimodalProjector)** - ç¬¬127-176è¡Œ

è¿™æ˜¯ **Vision ç‰¹å¾ â†’ LLM ç‰¹å¾** çš„æ¡¥æ¢ï¼š

```python
class MultimodalProjector(nn.Module):
    def __init__(self, config: VILAConfig):
        super().__init__()
        
        # VILA ç‰¹è‰²ï¼š3x3 ä¸‹é‡‡æ · + MLP
        self.layers = nn.Sequential(
            # 1. ç©ºé—´ä¸‹é‡‡æ ·ï¼š27x27 â†’ 9x9ï¼Œç‰¹å¾æ‹¼æ¥ (1152 â†’ 1152*9)
            DownSample3x3BlockFix(),
            
            # 2. å½’ä¸€åŒ– + é™ç»´
            nn.LayerNorm(config.mm_hidden_size * 9),  # 1152*9
            nn.Linear(config.mm_hidden_size * 9, config.mm_hidden_size * 3),
            nn.GELU(),
            
            # 3. å†æ¬¡é™ç»´åˆ° LLM ç»´åº¦
            nn.LayerNorm(config.vision_config.hidden_size * 3),
            nn.Linear(config.vision_config.hidden_size * 3, config.hidden_size),
            nn.GELU(),
            
            # 4. æœ€ç»ˆæŠ•å½±
            nn.Linear(config.hidden_size, config.hidden_size),  # 1536
        )
```

**DownSample3x3BlockFix è¯¦è§£** (ç¬¬93-124è¡Œ)ï¼š

```python
class DownSample3x3BlockFix(nn.Module):
    def forward(self, x: Tensor) -> Tensor:
        """
        è¾“å…¥: [batch, 729, 1152]  (27x27 patches)
        è¾“å‡º: [batch, 81, 10368]  (9x9 patches, æ¯ä¸ª patch åŒ…å« 9 ä¸ªåŸå§‹ patch)
        """
        batch, seq_len, hidden = x.shape
        feat_size = int(seq_len**0.5)  # 27
        
        # Reshape æˆ 2D feature map
        features = x.reshape(batch, feat_size, feat_size, hidden)
        # [batch, 27, 27, 1152]
        
        # Padding åˆ° 3 çš„å€æ•°
        pad = (3 - feat_size % 3) % 3
        if pad > 0:
            features = F.pad(features, (0, 0, 0, pad, 0, pad))
        
        # é‡æ’æˆ 3x3 å—
        features = features.reshape(batch, feat_size//3, 3, feat_size//3, 3, hidden)
        # [batch, 9, 3, 9, 3, 1152]
        
        features = features.permute(0, 1, 3, 2, 4, 5).contiguous()
        # [batch, 9, 9, 3, 3, 1152]
        
        # å±•å¹³ 3x3 å— â†’ æ‹¼æ¥ç‰¹å¾
        features = features.reshape(batch, -1, 9 * hidden)
        # [batch, 81, 10368]
        
        return features
```

**ä½œç”¨ï¼šå‡å°‘ token æ•°é‡ï¼ŒåŠ é€Ÿæ¨ç†ï¼**

---

### 3ï¸âƒ£ **ä¸»æ¨¡å‹ç±» (VILAForConditionalGeneration)** - ç¬¬182-306è¡Œ

```python
class VILAForConditionalGeneration(nn.Module):
    def __init__(self, config: VILAConfig, ...):
        super().__init__()
        
        # ä¸‰å¤§ç»„ä»¶
        self.vision_tower = SiglipVisionModel(config.vision_config)
        self.mm_projector = MultimodalProjector(config)
        self.llm = Qwen2ForCausalLM(config.text_config, ...)
        
        # è¾…åŠ©ç»„ä»¶
        self.logits_processor = LogitsProcessor(config)
        self.pooler = Pooler(pooling_type=PoolingType.LAST, normalize=True)
```

**æ ¸å¿ƒå‰å‘ä¼ æ’­æµç¨‹ï¼š**

```python
def forward(self, input_ids, positions, forward_batch, ...):
    # ä½¿ç”¨é€šç”¨çš„å¤šæ¨¡æ€èåˆå‡½æ•°
    output = mm_utils.general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        language_model=self.llm,
        data_embedding_funcs={
            Modality.IMAGE: self.get_image_feature,  # å›¾åƒå¤„ç†å‡½æ•°
        },
        positions=positions,
    )
    return output
```

**å›¾åƒç‰¹å¾æå– (get_image_feature)** - ç¬¬239-263è¡Œï¼š

```python
def get_image_feature(self, mm_input: List[MultimodalDataItem]) -> Tensor:
    pixel_values = mm_input[0].feature  # [batch, 3, 384, 384]
    
    # 1. Vision Encoder
    vision_output = self.vision_tower(
        pixel_values.to(device=self.vision_tower.device, dtype=...)
        output_hidden_states=True,  # è·å–æ‰€æœ‰å±‚çš„è¾“å‡º
    )
    # è¾“å‡º: hidden_states = [layer0, layer1, ..., layer_N]
    #      æ¯å±‚: [batch, 729, 1152]
    
    # 2. é€‰æ‹©ç‰¹å®šå±‚çš„è¾“å‡º
    mm_projector_input = self._vision_tower_output_to_mm_projector_input(
        vision_output
    )
    # é€‰æ‹©ç¬¬ -2 å±‚: [batch, 729, 1152]
    
    # 3. æŠ•å½±åˆ° LLM ç©ºé—´
    image_embedding = self.mm_projector(mm_projector_input)
    # è¾“å‡º: [batch, 81, 1536]
    
    return image_embedding
```

---

### 4ï¸âƒ£ **æƒé‡åŠ è½½ (load_weights)** - ç¬¬265-276è¡Œ

```python
def load_weights(self, weights: Iterable[Tuple[str, Tensor]]):
    params_dict = dict(self.named_parameters())
    
    for name, loaded_weight in weights:
        if name.startswith("llm."):
            # LLM æƒé‡å•ç‹¬å¤„ç†
            self.llm.load_weights([(name[len("llm."):], loaded_weight)])
        else:
            # Vision å’Œ Projector æƒé‡
            param = params_dict[name]
            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(param, loaded_weight)
```

---

### 5ï¸âƒ£ **å¤šæ¨¡æ€ Token å¡«å…… (pad_input_ids)** - ç¬¬278-282è¡Œ

```python
def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
    # ä½¿ç”¨é€šç”¨çš„å¤šæ¨¡æ€ token å¡«å……æ¨¡å¼
    pattern = MultiModalityDataPaddingPatternMultimodalTokens()
    return pattern.pad_input_tokens(input_ids, mm_inputs)
```

**ä½œç”¨ï¼š** å°† `<image>` å ä½ç¬¦æ›¿æ¢ä¸ºå®é™…çš„å›¾åƒ embedding ä½ç½®æ ‡è®°

---

## ğŸ”§ ä¸‰ã€å¤šæ¨¡æ€å¤„ç†å™¨ (Processor)

### VILAMultimodalProcessor (processors/vila.py)

```python
class VILAMultimodalProcessor(BaseMultimodalProcessor):
    models = [VILAForConditionalGeneration]  # æ³¨å†Œå…³è”æ¨¡å‹
    
    def __init__(self, hf_config, server_args, _processor, ...):
        super().__init__(...)
        
        # å®šä¹‰å¤šæ¨¡æ€ token
        self.mm_tokens = MultimodalSpecialTokens(
            image_token=_processor.tokenizer.image_token,  # "<image>"
            image_token_id=hf_config.image_token_id,      # 151649
            video_token_id=hf_config.video_token_id,      # 151650
        ).build(_processor)
```

**æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼š**

```python
async def process_mm_data_async(
    self,
    image_data: ImageDataInputItem | List[ImageDataInputItem],
    input_text: str | List[int],
    request_obj,
    **kwargs
):
    # 1. åŠ è½½å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾åƒ â†’ PIL Imageï¼‰
    base_output = self.load_mm_data(
        prompt=input_text,
        multimodal_tokens=self.mm_tokens,
        image_data=image_data,
    )
    
    # 2. å¤„ç†å¹¶ç»„åˆå¤šæ¨¡æ€æ•°æ®
    mm_items, input_ids, _ = self.process_and_combine_mm_data(
        base_output, self.mm_tokens
    )
    
    # 3. è¿”å›å¤„ç†ç»“æœ
    return {
        "input_ids": input_ids.tolist(),     # token IDs
        "mm_items": mm_items,                # å›¾åƒæ•°æ®
        "im_token_id": self.mm_tokens.image_token_id,
        "video_token_id": self.mm_tokens.video_token_id,
    }
```

---

## ğŸ§ª å››ã€æµ‹è¯• (test_vision_openai_server_a.py)

```python
class TestVILAServer(ImageOpenAITestMixin):
    @classmethod
    def setUpClass(cls):
        # 1. æŒ‡å®šæ¨¡å‹
        cls.model = "Efficient-Large-Model/NVILA-Lite-2B-hf-0626"
        cls.base_url = DEFAULT_URL_FOR_TEST
        cls.revision = "6bde1de5964b40e61c802b375fff419edc867506"
        
        # 2. å¯åŠ¨æœåŠ¡å™¨
        cls.process = popen_launch_server(
            cls.model,
            cls.base_url,
            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
            other_args=[
                "--revision", cls.revision,
                "--trust-remote-code",
            ],
        )
```

**æµ‹è¯•å†…å®¹ï¼ˆç»§æ‰¿è‡ª ImageOpenAITestMixinï¼‰ï¼š**
- âœ… å•å›¾åƒç”Ÿæˆæµ‹è¯•
- âœ… å¤šå›¾åƒç”Ÿæˆæµ‹è¯•
- âœ… OpenAI API å…¼å®¹æ€§æµ‹è¯•
- âœ… æµå¼è¾“å‡ºæµ‹è¯•

---

## ğŸ“Š äº”ã€æ•°æ®æµå…¨æµç¨‹

### å®Œæ•´çš„æ¨ç†æµç¨‹ï¼š

```python
# ç”¨æˆ·è¾“å…¥
{
    "messages": [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "base64_encoded_image"},
                {"type": "text", "text": "What's in this image?"}
            ]
        }
    ]
}

â†“ (1) VILAMultimodalProcessor.process_mm_data_async()

{
    "input_ids": [151649, 1234, 5678, ...],  # 151649 æ˜¯ <image> token
    "mm_items": [MultimodalDataItem(feature=pixel_values)],
}

â†“ (2) VILAForConditionalGeneration.forward()

# æ£€æµ‹åˆ° image_token_id (151649)
â†“

â†“ (3) get_image_feature()

pixel_values [1, 3, 384, 384]
    â†“ SiglipVisionModel
hidden_states[-2] [1, 729, 1152]
    â†“ MultimodalProjector
image_embedding [1, 81, 1536]

â†“ (4) general_mm_embed_routine()

# å°† image_embedding æ’å…¥åˆ° input_ids å¯¹åº”ä½ç½®
combined_embeds = [text_embed, image_embed, text_embed, ...]

â†“ (5) Qwen2ForCausalLM.forward()

combined_embeds [1, total_seq_len, 1536]
    â†“ Transformer Layers
hidden_states [1, total_seq_len, 1536]
    â†“ LM Head
logits [1, total_seq_len, vocab_size]

â†“ (6) LogitsProcessor

# é‡‡æ · â†’ ç”Ÿæˆæ–‡æœ¬
output_text = "This image shows a cat sitting on a table..."
```

---

## ğŸ¯ å…­ã€å…³é”®çŸ¥è¯†ç‚¹æ€»ç»“

### âœ… è®¾è®¡äº®ç‚¹

| ç‰¹æ€§ | å®ç°æ–¹å¼ | ä¼˜åŠ¿ |
|------|----------|------|
| **æ¨¡å—åŒ–** | Vision/Projector/LLM ç‹¬ç«‹ | æ˜“äºæ›¿æ¢ç»„ä»¶ |
| **Token å‹ç¼©** | 3x3 ä¸‹é‡‡æ · (729â†’81) | æ¨ç†é€Ÿåº¦æå‡ 9å€ |
| **å±‚é€‰æ‹©** | ä½¿ç”¨ ViT çš„ç¬¬ -2 å±‚ | æ›´ä¸°å¯Œçš„ç‰¹å¾ |
| **é€šç”¨èåˆ** | `general_mm_embed_routine()` | ä»£ç å¤ç”¨ |

### ğŸ“ å®ç° Ovis çš„æ”¹åŠ¨ç‚¹

æ ¹æ® VILA å®ç° Ovisï¼Œä½ éœ€è¦æ”¹ï¼š

```python
# 1. models/ovis.py
class Ovis(nn.Module):
    def __init__(self, config, ...):
        # âœ… å¤ç”¨
        self.visual_tokenizer = SiglipVisionModel(...)  
        
        # âš ï¸ å¯èƒ½ä¸åŒ
        self.mm_projector = OvisProjector(...)  # å¯èƒ½æ˜¯ç®€å• MLP
        
        # âš ï¸ æ”¹ä¸º Qwen3
        self.llm = Qwen3ForCausalLM(...)  # è€Œé Qwen2
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- VILA æ¨¡å‹: `python/sglang/srt/models/vila.py`
- VILA å¤„ç†å™¨: `python/sglang/srt/multimodal/processors/vila.py`
- VILA æµ‹è¯•: `test/srt/test_vision_openai_server_a.py`
- SigLIP å®ç°: `python/sglang/srt/models/siglip.py`
- é€šç”¨èåˆå‡½æ•°: `python/sglang/srt/managers/mm_utils.py`

