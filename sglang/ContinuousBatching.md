# SGLang Continuous Batching æ·±åº¦è§£æ

> åŸºäº sglang æºç çš„ Continuous Batching å®Œå…¨æŒ‡å—

---

## ğŸ“š ç›®å½•

1. [ä»€ä¹ˆæ˜¯ Continuous Batchingï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-continuous-batching)
2. [æ ¸å¿ƒæ•°æ®ç»“æ„](#2-æ ¸å¿ƒæ•°æ®ç»“æ„)
3. [ä¸»äº‹ä»¶å¾ªç¯](#3-ä¸»äº‹ä»¶å¾ªç¯)
4. [Decode é˜¶æ®µçš„ Continuous Batching](#4-decode-é˜¶æ®µçš„-continuous-batching)
5. [Prefill é˜¶æ®µçš„æ‰¹å¤„ç†](#5-prefill-é˜¶æ®µçš„æ‰¹å¤„ç†)
6. [Prefill vs Decode çš„åŒºåˆ«](#6-prefill-vs-decode-çš„åŒºåˆ«)
7. [Chunked Prefill æœºåˆ¶](#7-chunked-prefill-æœºåˆ¶)
8. [æµé‡æ§åˆ¶ä¸è¿‡è½½ä¿æŠ¤](#8-æµé‡æ§åˆ¶ä¸è¿‡è½½ä¿æŠ¤)
9. [å…³é”®é…ç½®å‚æ•°](#9-å…³é”®é…ç½®å‚æ•°)
10. [å®Œæ•´æµç¨‹ç¤ºä¾‹](#10-å®Œæ•´æµç¨‹ç¤ºä¾‹)

---

## 1. ä»€ä¹ˆæ˜¯ Continuous Batchingï¼Ÿ

### 1.1 å®šä¹‰

**Continuous Batching** æ˜¯ä¸€ç§åŠ¨æ€æ‰¹å¤„ç†æŠ€æœ¯ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ– LLM æ¨ç†æœåŠ¡çš„ååé‡ã€‚ä¸ä¼ ç»Ÿæ‰¹å¤„ç†ä¸åŒï¼š

- âŒ **ä¼ ç»Ÿæ‰¹å¤„ç†**ï¼šç­‰å¾…ä¸€æ‰¹è¯·æ±‚å…¨éƒ¨å®Œæˆæ‰èƒ½å¤„ç†ä¸‹ä¸€æ‰¹
- âœ… **Continuous Batching**ï¼šå½“æ‰¹æ¬¡ä¸­æœ‰è¯·æ±‚å®Œæˆæ—¶ï¼Œç«‹å³ç”¨æ–°è¯·æ±‚å¡«å……ç©ºä½

### 1.2 æ ¸å¿ƒæ€æƒ³

```
ç»´æŠ¤ä¸€ä¸ª batch_size å¤§å°çš„"æ± å­"
   â†“
è½®è¯¢å¼æ‰§è¡Œ (æ¯è½®ç”Ÿæˆ 1 token)
   â†“
æ¯è½®å checkï¼šæ˜¯å¦æœ‰è¯·æ±‚å®Œæˆï¼Ÿ
   â†“
ç§»é™¤å®Œæˆçš„è¯·æ±‚ï¼Œç«‹å³å¡å…¥æ–°è¯·æ±‚
   â†“
å¾ªç¯å¾€å¤ï¼ŒGPU æ°¸ä¸ç©ºé—²
```

### 1.3 ä¼˜åŠ¿

1. **é›¶ç­‰å¾…æ—¶é—´**ï¼šè¯·æ±‚å®Œæˆç«‹å³è¢«æ–°è¯·æ±‚æ›¿æ¢
2. **æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡**ï¼šé¿å…ç©ºé—²ç­‰å¾…
3. **é™ä½å¹³å‡å»¶è¿Ÿ**ï¼šæ–°è¯·æ±‚ä¸éœ€è¦ç­‰å¾…æ•´ä¸ªæ‰¹æ¬¡å®Œæˆ
4. **æé«˜ååé‡**ï¼šæ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›

---

## 2. æ ¸å¿ƒæ•°æ®ç»“æ„

### 2.1 ç­‰å¾…é˜Ÿåˆ—ï¼ˆPrefill Queueï¼‰

```python
# python/sglang/srt/managers/scheduler.py:517
self.waiting_queue: List[Req] = []
```

- **ç±»å‹**ï¼šç®€å•çš„ Python List
- **ä½œç”¨**ï¼šå­˜å‚¨æ‰€æœ‰ç­‰å¾… prefill çš„è¯·æ±‚
- **æ’åº**ï¼šå¯æ ¹æ®è°ƒåº¦ç­–ç•¥æ’åºï¼ˆFCFSã€LPMã€DFS-Weight ç­‰ï¼‰

### 2.2 è¿è¡Œæ‰¹æ¬¡ï¼ˆRunning Batchï¼‰

```python
# python/sglang/srt/managers/scheduler.py:519
self.running_batch: ScheduleBatch = ScheduleBatch(reqs=[], batch_is_full=False)
```

- **ç±»å‹**ï¼š`ScheduleBatch` å¯¹è±¡
- **ä½œç”¨**ï¼šå½“å‰æ­£åœ¨ decode çš„è¯·æ±‚æ± 
- **åŠ¨æ€è°ƒæ•´**ï¼šæ¯è½® decode åè¿‡æ»¤å®Œæˆçš„è¯·æ±‚

### 2.3 è¯·æ±‚å¯¹è±¡ï¼ˆReqï¼‰

```python
# python/sglang/srt/managers/schedule_batch.py:434
class Req:
    rid: str                    # è¯·æ±‚ ID
    origin_input_ids: List[int] # åŸå§‹è¾“å…¥ tokens
    output_ids: List[int]       # å·²ç”Ÿæˆçš„ tokens
    sampling_params: SamplingParams
    finished_reason: Optional[str]  # å®ŒæˆåŸå› 
```

---

## 3. ä¸»äº‹ä»¶å¾ªç¯

### 3.1 æ ¸å¿ƒå¾ªç¯ä»£ç 

```python
# python/sglang/srt/managers/scheduler.py:979-995
def event_loop_normal(self):
    """A normal scheduler loop."""
    while True:  # â† æ— é™å¾ªç¯ï¼
        # 1ï¸âƒ£ æ¥æ”¶æ–°çš„å¹¶å‘è¯·æ±‚
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)  # åŠ å…¥ waiting_queue
        
        # 2ï¸âƒ£ è·å–ä¸‹ä¸€ä¸ªè¦è¿è¡Œçš„ batch
        batch = self.get_next_batch_to_run()
        self.cur_batch = batch
        
        if batch:
            # 3ï¸âƒ£ è¿è¡Œä¸€è½®ï¼ˆprefill æˆ– decodeï¼‰
            result = self.run_batch(batch)
            
            # 4ï¸âƒ£ å¤„ç†ç»“æœï¼šcheck æ˜¯å¦å®Œæˆï¼Œå†³å®šæ˜¯å¦å¡æ–° req
            self.process_batch_result(batch, result)
        else:
            # ç©ºé—²æ—¶è‡ªæ£€
            self.self_check_during_idle()
        
        self.last_batch = batch
```

### 3.2 å¾ªç¯æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Event Loop (æ— é™å¾ªç¯)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â†’ recv_requests() â”€â†’ waiting_queue
     â”‚
     â”œâ”€â†’ get_next_batch_to_run()
     â”‚      â”œâ”€â†’ ä¼˜å…ˆ prefill (new_batch)
     â”‚      â””â”€â†’ å¦åˆ™ decode (running_batch)
     â”‚
     â”œâ”€â†’ run_batch() â”€â†’ GPU å‰å‘ä¼ æ’­
     â”‚
     â”œâ”€â†’ process_batch_result()
     â”‚      â”œâ”€â†’ filter_batch() (ç§»é™¤å®Œæˆçš„è¯·æ±‚)
     â”‚      â””â”€â†’ merge æ–° prefill è¯·æ±‚
     â”‚
     â””â”€â†’ å›åˆ°å¾ªç¯å¼€å§‹
```

---

## 4. Decode é˜¶æ®µçš„ Continuous Batching

### 4.1 åŠ¨æ€æ›´æ–°æ‰¹æ¬¡

```python
# python/sglang/srt/managers/scheduler.py:2016-2060
def update_running_batch(self, batch: ScheduleBatch) -> Optional[ScheduleBatch]:
    """Update the current running decoding batch."""
    initial_bs = batch.batch_size()
    
    # ğŸ”¥ å…³é”®ï¼šç§»é™¤æ‰€æœ‰å·²å®Œæˆçš„è¯·æ±‚
    batch.filter_batch()
    if batch.is_empty():
        batch.batch_is_full = False
        return batch
    
    # æ£€æŸ¥å†…å­˜ã€å¤„ç† retract ç­‰...
    
    if batch.batch_size() < initial_bs:
        batch.batch_is_full = False  # ğŸ”¥ æ‰¹æ¬¡æœ‰ç©ºä½äº†ï¼
    
    # æ›´æ–°æ‰¹æ¬¡å¼ é‡ï¼Œå‡†å¤‡ä¸‹ä¸€è½® decode
    batch.prepare_for_decode()
    return batch
```

### 4.2 è¿‡æ»¤å·²å®Œæˆè¯·æ±‚

```python
# python/sglang/srt/managers/schedule_batch.py:1720-1757
def filter_batch(
    self,
    chunked_req_to_exclude: Optional[Union[Req, List[Req]]] = None,
    keep_indices: Optional[List[int]] = None,
):
    if keep_indices is None:
        # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šåªä¿ç•™æœªå®Œæˆçš„è¯·æ±‚
        keep_indices = [
            i
            for i in range(len(self.reqs))
            if not self.reqs[i].finished()  # â† å…³é”®åˆ¤æ–­
            and self.reqs[i] not in chunked_req_to_exclude
        ]
    
    # æ ¹æ® keep_indices è¿‡æ»¤æ‰€æœ‰å¼ é‡
    self.reqs = [self.reqs[i] for i in keep_indices]
    self.req_pool_indices = self.req_pool_indices[keep_indices_device]
    self.seq_lens = self.seq_lens[keep_indices_device]
    # ... è¿‡æ»¤å…¶ä»–å¼ é‡ ...
```

### 4.3 æ£€æŸ¥å®Œæˆæ¡ä»¶

```python
# python/sglang/srt/managers/scheduler_output_processor_mixin.py:200-259
def process_batch_result_decode(
    self: Scheduler,
    batch: ScheduleBatch,
    result: GenerationBatchResult,
):
    # éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªè¯·æ±‚
    for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
        if req.is_retracted:
            continue
        
        # æ·»åŠ ç”Ÿæˆçš„ token
        req.output_ids.append(next_token_id)
        
        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦å®Œæˆ
        req.check_finished()
        if req.finished():
            # å°† KV cache ç¼“å­˜èµ·æ¥ä»¥ä¾¿å¤ç”¨
            self.tree_cache.cache_finished_req(req)
            req.time_stats.completion_time = time.perf_counter()
```

### 4.4 Decode æµç¨‹ç¤ºä¾‹

```
ã€åˆå§‹çŠ¶æ€ã€‘
waiting_queue: [Req4, Req5]
running_batch: [Req1(token 1/50), Req2(token 1/10), Req3(token 1/100)]
batch_size = 3 (æ± å­å¤§å°)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€ç¬¬ 1-9 è½® Decodeã€‘
æ¯è½®ï¼šrun_batch() â†’ GPU å¹¶è¡Œç”Ÿæˆ 1 token
ç»“æœï¼šæ‰€æœ‰éƒ½æœªå®Œæˆ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€ç¬¬ 10 è½® Decodeã€‘
run_batch() â†’ GPU å¹¶è¡Œç”Ÿæˆï¼š
  - Req1: ç”Ÿæˆ token 11
  - Req2: ç”Ÿæˆ token 11 [EOS] â† å®Œæˆäº†ï¼
  - Req3: ç”Ÿæˆ token 11

check: filter_batch()
  â†’ Req2.finished() == True
  â†’ ç§»é™¤ Req2 âœ…

running_batch: [Req1(11/50), Req3(11/100)]  â† æ± å­æœ‰ç©ºä½äº†ï¼
batch_is_full = False

get_new_batch_prefill():
  â†’ ä» waiting_queue å–å‡º Req4
  â†’ åš prefill

merge_batch():
  â†’ running_batch += Req4

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€ç¬¬ 11 è½® Decodeã€‘
running_batch: [Req1(11/50), Req3(11/100), Req4(1/20)]  â† å¡è¿›æ¥äº†ï¼
batch_size = 3 (æ± å­åˆæ»¡äº†)

run_batch(MIXED) â†’ ç»§ç»­è½®è¯¢...
```

---

## 5. Prefill é˜¶æ®µçš„æ‰¹å¤„ç†

### 5.1 è·å– Prefill Batch

```python
# python/sglang/srt/managers/scheduler.py:1867-1951
def get_new_batch_prefill(self):
    # 1ï¸âƒ£ å¯¹ waiting_queue æ’åºï¼ˆæ ¹æ®è°ƒåº¦ç­–ç•¥ï¼‰
    self.policy.calc_priority(self.waiting_queue)
    
    # 2ï¸âƒ£ åˆ›å»º PrefillAdder æ¥ç®¡ç†èµ„æºé¢„ç®—
    adder = PrefillAdder(
        self.page_size,
        self.tree_cache,
        self.token_to_kv_pool_allocator,
        self.running_batch,
        self.new_token_ratio,
        self.max_prefill_tokens,  # â† é™åˆ¶ï¼šæœ€å¤§ prefill tokens
        self.chunked_prefill_size,
        running_bs if self.is_mixed_chunk else 0,
    )
    
    # 3ï¸âƒ£ è´ªå¿ƒåœ°ä» waiting_queue ä¸­å–å‡ºè¯·æ±‚
    for req in self.waiting_queue:
        # å„ç§æ£€æŸ¥ï¼šLoRAã€batch_sizeã€å†…å­˜...
        
        # å°è¯•æ·»åŠ è¿™ä¸ªè¯·æ±‚
        res = adder.add_one_req(req, ...)
        
        if res != AddReqResult.CONTINUE:
            # ğŸ”¥ ç¬¬ä¸€æ¬¡å¤±è´¥å°±åœæ­¢æ·»åŠ 
            break
    
    # 4ï¸âƒ£ è·å–æ‰€æœ‰èƒ½è¿è¡Œçš„è¯·æ±‚
    can_run_list = adder.can_run_list  # å¯èƒ½æœ‰å¤šä¸ªï¼
    
    # 5ï¸âƒ£ ä» waiting_queue ä¸­ç§»é™¤è¿™äº›è¯·æ±‚
    self.waiting_queue = [
        x for x in self.waiting_queue 
        if x not in set(can_run_list)
    ]
    
    # 6ï¸âƒ£ åˆ›å»ºä¸€ä¸ª batchï¼ŒåŒ…å«å¤šä¸ª prefill è¯·æ±‚
    new_batch = ScheduleBatch.init_new(can_run_list, ...)
    
    return new_batch
```

### 5.2 å‡†å¤‡ Prefill Batch

```python
# python/sglang/srt/managers/schedule_batch.py:1227-1252
def prepare_for_extend(self):
    self.forward_mode = ForwardMode.EXTEND
    
    # ğŸ”¥ å…³é”®ï¼šæå–æ¯ä¸ªè¯·æ±‚çš„å…¨éƒ¨è¾“å…¥ tokens
    input_ids = [r.fill_ids[len(r.prefix_indices):] for r in reqs]
    
    # ğŸ”¥ è®¡ç®—æ‰€æœ‰ tokens çš„æ€»å’Œ
    extend_num_tokens = sum(len(ids) for ids in input_ids)
    
    seq_lens = [len(r.fill_ids) for r in reqs]
    prefix_lens = [len(r.prefix_indices) for r in reqs]
    extend_lens = [r.extend_input_len for r in reqs]
    
    # ğŸ”¥ å°†æ‰€æœ‰ tokens æ‹¼æ¥æˆä¸€ä¸ªå¤§ tensor
    input_ids_tensor = torch.tensor(
        list(chain.from_iterable(input_ids)), dtype=torch.int64
    ).to(self.device, non_blocking=True)
```

### 5.3 æ‰§è¡Œ Prefill

```python
# python/sglang/srt/model_executor/model_runner.py:1914-1935
def forward_extend(
    self,
    forward_batch: ForwardBatch,
    skip_attn_backend_init: bool = False,
    pp_proxy_tensors=None,
) -> LogitsProcessorOutput:
    if not skip_attn_backend_init:
        self.attn_backend.init_forward_metadata(forward_batch)
    
    # ğŸ”¥ ä¸€æ¬¡æ€§è°ƒç”¨ model.forwardï¼Œå¤„ç†æ‰€æœ‰ tokens
    return self.model.forward(
        forward_batch.input_ids,  # åŒ…å«æ‰€æœ‰è¯·æ±‚çš„æ‰€æœ‰ tokens
        forward_batch.positions,
        forward_batch,
        **kwargs,
    )
```

### 5.4 Prefill ç¤ºä¾‹

```
ã€åˆå§‹çŠ¶æ€ã€‘
waiting_queue = [
    Req1 (input: 100 tokens),
    Req2 (input: 200 tokens),
    Req3 (input: 150 tokens),
    Req4 (input: 300 tokens),
    Req5 (input: 50 tokens),
]

å‡è®¾é™åˆ¶ï¼š
- max_prefill_tokens = 500
- max_running_requests = 32

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€è°ƒåº¦è¿‡ç¨‹ã€‘

1ï¸âƒ£ calc_priority(waiting_queue) â†’ æ’åº

2ï¸âƒ£ PrefillAdder è´ªå¿ƒæ·»åŠ ï¼š
   
   å°è¯• Req1 (100 tokens):
   âœ… ç´¯è®¡ 100 tokens â‰¤ 500
   â†’ can_run_list = [Req1]
   
   å°è¯• Req2 (200 tokens):
   âœ… ç´¯è®¡ 300 tokens â‰¤ 500
   â†’ can_run_list = [Req1, Req2]
   
   å°è¯• Req3 (150 tokens):
   âœ… ç´¯è®¡ 450 tokens â‰¤ 500
   â†’ can_run_list = [Req1, Req2, Req3]
   
   å°è¯• Req4 (300 tokens):
   âŒ ç´¯è®¡ 750 tokens > 500  â† è¶…é™ï¼
   â†’ STOP (ç¬¬ä¸€æ¬¡å¤±è´¥å°±åœæ­¢)
   
3ï¸âƒ£ åˆ›å»º batchï¼š
   new_batch = ScheduleBatch([Req1, Req2, Req3])
   forward_mode = EXTEND
   
4ï¸âƒ£ æ›´æ–°é˜Ÿåˆ—ï¼š
   waiting_queue = [Req4, Req5]  â† å‰©ä¸‹çš„ç»§ç»­ç­‰å¾…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€GPU æ‰§è¡Œã€‘

run_batch(new_batch) â†’ ä¸€æ¬¡æ€§å¹¶è¡Œå¤„ç† 3 ä¸ª prefillï¼

æ³¨æ„ï¼šæŸäº› attention backend ä¼šä¸²è¡Œå¤„ç†æ¯ä¸ªåºåˆ—ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  for seq_idx in range(3):          â”‚
â”‚    if seq_idx == 0:                â”‚
â”‚      process Req1's 100 tokens     â”‚ â† ä¸€å£æ°”
â”‚    if seq_idx == 1:                â”‚
â”‚      process Req2's 200 tokens     â”‚ â† ä¸€å£æ°”
â”‚    if seq_idx == 2:                â”‚
â”‚      process Req3's 150 tokens     â”‚ â† ä¸€å£æ°”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ€»æ—¶é—´ â‰ˆ max(100, 200, 150) + overhead
ğŸ”¥ æœ€é•¿çš„ Req2 åˆ¶çº¦äº†æ€»æ—¶é—´ï¼

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ã€Prefill å®Œæˆåã€‘

process_batch_result():
  â†’ Req1, Req2, Req3 çš„è¾“å…¥éƒ½å¤„ç†å®Œæ¯•
  â†’ åˆå¹¶åˆ° running_batch è¿›å…¥ decode é˜¶æ®µ

running_batch = [Req1(decode), Req2(decode), Req3(decode)]
```

---

## 6. Prefill vs Decode çš„åŒºåˆ«

### 6.1 æ ¸å¿ƒå·®å¼‚å¯¹æ¯”

| ç‰¹æ€§ | Prefill é˜¶æ®µ | Decode é˜¶æ®µ |
|-----|-------------|------------|
| **å¤„ç†æ–¹å¼** | ä¸€å£æ°”å¤„ç†æ•´ä¸ªè¾“å…¥ | æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª token |
| **æ¯æ¬¡å¤„ç† tokens** | å‡ ååˆ°å‡ åƒ | **1 ä¸ª** |
| **æ‰§è¡Œæ¬¡æ•°** | 1 æ¬¡ï¼ˆé™¤é chunkedï¼‰ | å¤šæ¬¡ï¼ˆmax_new_tokensï¼‰ |
| **æŒç»­æ—¶é—´** | çŸ­ï¼ˆå‡ ç™¾æ¯«ç§’ï¼‰ | **é•¿ï¼ˆå‡ ç§’åˆ°å‡ åç§’ï¼‰** |
| **å®Œæˆæ—¶é—´å·®å¼‚** | å°ï¼ˆç±»ä¼¼è¾“å…¥é•¿åº¦ï¼‰ | **å¤§ï¼ˆä¸åŒ max_new_tokensï¼‰** |
| **åŠ¨æ€è°ƒæ•´é¢‘ç‡** | ä½ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰ | **é«˜ï¼ˆæ¯æ¬¡è¿­ä»£ï¼‰** |
| **Continuous Batching æ”¶ç›Š** | ä¸­ç­‰ | **éå¸¸é«˜** |
| **æ˜¯å¦æ˜¯ç“¶é¢ˆ** | å¦ | **æ˜¯** |
| **æ‰¹å¤„ç†ç­–ç•¥** | æ‰¹é‡æ‰“åŒ…ï¼Œå¹¶è¡Œå¤„ç† | åŠ¨æ€å¢åˆ ï¼Œè½®è¯¢æ‰§è¡Œ |

### 6.2 ä¸ºä»€ä¹ˆ Decode æ›´éœ€è¦ Continuous Batchingï¼Ÿ

#### åŸå›  1ï¼šé€ Token ç”Ÿæˆï¼ŒæŒç»­æ—¶é—´é•¿

```python
# python/sglang/srt/managers/schedule_batch.py:1463-1464
for req in running_batch.reqs:
    req.extend_input_len = 1  # ğŸ”¥ decode æ¯æ¬¡åªå¤„ç† 1 ä¸ª token
```

#### åŸå›  2ï¼šå®Œæˆæ—¶é—´ä¸åŒæ­¥

```python
# python/sglang/srt/managers/schedule_batch.py:735-783
def check_finished(self):
    # æ£€æŸ¥å¤šç§å®Œæˆæ¡ä»¶
    if len(self.output_ids) >= self.sampling_params.max_new_tokens:
        self.finished_reason = FINISH_LENGTH(...)
        return
    
    if last_token_id in stop_token_ids:
        self.finished_reason = FINISH_MATCHED_TOKEN(...)
        return
```

æ¯ä¸ªè¯·æ±‚çš„ `max_new_tokens` ä¸åŒï¼ŒEOS token å‡ºç°æ—¶é—´ä¸åŒï¼Œå¯¼è‡´å®Œæˆæ—¶é—´å‚å·®ä¸é½ã€‚

#### åŸå›  3ï¼šè®¡ç®—é‡å°ï¼Œå®¹æ˜“æœ‰ç©ºä½

Decode æ¯æ¬¡åªå¤„ç† 1 tokenï¼Œè®¡ç®—é‡å°ï¼Œæ›´å®¹æ˜“åœ¨æ‰¹æ¬¡ä¸­è…¾å‡ºç©ºé—´åŠ å…¥æ–°è¯·æ±‚ã€‚

---

## 7. Chunked Prefill æœºåˆ¶

### 7.1 è§¦å‘æ¡ä»¶

**Chunked Prefill** å’Œ **max_prefill_tokens** æ˜¯**ä¸¤ä¸ªç‹¬ç«‹çš„æœºåˆ¶**ï¼

```python
# python/sglang/srt/managers/schedule_policy.py:593-637
def add_one_req(self, req: Req, ...):
    # åˆ¤æ–­ï¼šæ˜¯å¦å¯ç”¨ chunked prefill
    if self.rem_chunk_tokens is None or input_tokens <= self.rem_chunk_tokens:
        # âœ… ä¸éœ€è¦ chunked prefillï¼ˆæ­£å¸¸å¤„ç†ï¼‰
        self.can_run_list.append(req)
        self._update_prefill_budget(...)
    else:
        # âŒ éœ€è¦ chunked prefillï¼ˆå•ä¸ªè¯·æ±‚è¾“å…¥å¤ªé•¿ï¼‰
        trunc_len = self.rem_chunk_tokens // self.page_size * self.page_size
        if trunc_len <= 0:
            return AddReqResult.OTHER
        
        # ğŸ”¥ æˆªæ–­è¾“å…¥ï¼Œåˆ†å—å¤„ç†
        req.extend_input_len = trunc_len
        req.fill_ids = req.fill_ids[: len(req.prefix_indices) + trunc_len]
        
        self.can_run_list.append(req)
        self.new_chunked_req = req  # æ ‡è®°ä¸º chunked è¯·æ±‚
        self._update_prefill_budget(prefix_len, trunc_len, 0)
```

### 7.2 ä¸¤ä¸ªå‚æ•°çš„åŒºåˆ«

| å‚æ•° | ä½œç”¨èŒƒå›´ | é™åˆ¶å†…å®¹ | è¶…é™åçš„è¡Œä¸º |
|-----|---------|---------|------------|
| **max_prefill_tokens** | æ•´ä¸ª batch | æ‰€æœ‰è¯·æ±‚çš„ tokens æ€»å’Œ | åœæ­¢æ‰“åŒ…ï¼Œå‰©ä½™è¯·æ±‚ä¸‹æ¬¡å¤„ç† |
| **chunked_prefill_size** | å•ä¸ªè¯·æ±‚ | å•ä¸ªè¯·æ±‚çš„ tokens æ•°é‡ | åˆ†å—å¤„ç†è¯¥è¯·æ±‚ï¼ˆå¤šè½® prefillï¼‰ |

### 7.3 åœºæ™¯å¯¹æ¯”

#### åœºæ™¯ Aï¼šè¶…è¿‡ max_prefill_tokens

```
waiting_queue = [Req1(100), Req2(200), Req3(300)]
max_prefill_tokens = 400
chunked_prefill_size = None (ç¦ç”¨)

æ‰“åŒ…ï¼š
  âœ… Req1 (100) â†’ ç´¯è®¡ 100
  âœ… Req2 (200) â†’ ç´¯è®¡ 300
  âŒ Req3 (300) â†’ ç´¯è®¡ 600 > 400 â† è¶…é™ï¼
  â†’ break

ç»“æœï¼šè¿™æ¬¡å¤„ç† [Req1, Req2]ï¼ŒReq3 ä¸‹æ¬¡å†å¤„ç†
â†’ ä¸æ˜¯ chunked prefillï¼Œåªæ˜¯å»¶åå¤„ç†
```

#### åœºæ™¯ Bï¼šè§¦å‘ chunked_prefill

```
waiting_queue = [Req1(5000 tokens)]
max_prefill_tokens = 8192
chunked_prefill_size = 2048  â† å¯ç”¨ï¼

æ‰“åŒ…ï¼š
  å°è¯• Req1 (5000):
    5000 > 2048 â† è§¦å‘ chunked prefillï¼
    
    ç¬¬ä¸€è½®ï¼šå¤„ç† tokens [0, 2048)
    ç¬¬äºŒè½®ï¼šå¤„ç† tokens [2048, 4096)
    ç¬¬ä¸‰è½®ï¼šå¤„ç† tokens [4096, 5000)

ç»“æœï¼šä¸€ä¸ªå¤§è¯·æ±‚è¢«æ‹†æˆ 3 æ¬¡ prefill
```

---

## 8. æµé‡æ§åˆ¶ä¸è¿‡è½½ä¿æŠ¤

### 8.1 è¯·æ±‚æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client (HTTP)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1ï¸âƒ£ TCP è¿æ¥
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HTTP Server                â”‚
â”‚  (FastAPI/uvicorn)          â”‚
â”‚  - max_connections é™åˆ¶     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 2ï¸âƒ£ å¼‚æ­¥äº‹ä»¶å¾ªç¯
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TokenizerManager           â”‚
â”‚  (asyncio event loop)       â”‚
â”‚  - å¼‚æ­¥é˜Ÿåˆ—                 â”‚
â”‚  - ä¸ä¼šæ— é™å¢é•¿             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 3ï¸âƒ£ ZMQ pipe
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler                  â”‚
â”‚  - waiting_queue: List[Req] â”‚ â† ğŸ”¥ æ ¸å¿ƒé˜»å¡ç‚¹
â”‚  - max_queued_requests é™åˆ¶ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 é˜Ÿåˆ—æº¢å‡ºä¿æŠ¤

```python
# python/sglang/srt/managers/scheduler.py:1488-1568
def _add_request_to_queue(self, req: Req, is_retracted: bool = False):
    if self.disaggregation_mode == DisaggregationMode.NULL:
        self._set_or_validate_priority(req)
        
        # ğŸ”¥ å…³é”®ï¼šæ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦æ»¡
        if self._abort_on_queued_limit(req):
            return  # æ‹’ç»è¿™ä¸ªè¯·æ±‚
        
        self._prefetch_kvcache(req)
        self.waiting_queue.append(req)

def _abort_on_queued_limit(self, recv_req: Req) -> bool:
    """Abort an incoming or existing request if the waiting queue is full."""
    
    # æ£€æŸ¥é˜Ÿåˆ—å¤§å°
    if (
        self.max_queued_requests is None
        or len(self.waiting_queue) + 1 <= self.max_queued_requests
    ):
        return False  # é˜Ÿåˆ—è¿˜æœ‰ç©ºé—´
    
    # ğŸ”¥ é˜Ÿåˆ—æ»¡äº†ï¼
    req_to_abort = recv_req
    message = "The request queue is full."
    
    if self.enable_priority_scheduling:
        # ğŸ”¥ ä¼˜å…ˆçº§è°ƒåº¦ï¼šå¯èƒ½è¸¢æ‰ä½ä¼˜å…ˆçº§çš„æ—§è¯·æ±‚
        direction = 1 if self.schedule_low_priority_values_first else -1
        key_fn = lambda item: (
            direction * item[1].priority,
            item[1].time_stats.wait_queue_entry_time,
        )
        idx, candidate_req = max(enumerate(self.waiting_queue), key=key_fn)
        abort_existing_req = (
            direction * recv_req.priority < direction * candidate_req.priority
        )
        if abort_existing_req:
            self.waiting_queue.pop(idx)  # è¸¢æ‰æ—§è¯·æ±‚
            req_to_abort = candidate_req
            message = "The request is aborted by a higher priority request."
    
    # ğŸ”¥ å‘é€ abort æ¶ˆæ¯
    self.send_to_tokenizer.send_pyobj(
        AbortReq(
            finished_reason={
                "type": "abort",
                "status_code": HTTPStatus.SERVICE_UNAVAILABLE,  # 503
                "message": message,
            },
            rid=req_to_abort.rid,
        )
    )
    return req_to_abort.rid == recv_req.rid
```

### 8.3 ä¸‰å±‚é˜²æŠ¤æœºåˆ¶

| å±‚çº§ | ç»„ä»¶ | ä¿æŠ¤æœºåˆ¶ | è¶…é™è¡Œä¸º |
|-----|------|---------|---------|
| **Level 1** | HTTP Server | `max_connections` | æ‹’ç» TCP è¿æ¥ |
| **Level 2** | TokenizerManager | asyncio å¼‚æ­¥é˜Ÿåˆ— | è‡ªç„¶èƒŒå‹ |
| **Level 3** | Scheduler | `max_queued_requests` | è¿”å› 503 é”™è¯¯ |

### 8.4 å®é™…è¡¨ç°

```python
# é…ç½®ç¤ºä¾‹
server_args = {
    "max_queued_requests": 1000,
    "max_prefill_tokens": 8192,
    "chunked_prefill_size": 4096,
}

# åœºæ™¯ï¼šç–¯ç‹‚å‘è¯·æ±‚
for i in range(10000):
    requests.post("/v1/completions", ...)

# ç»“æœï¼š
#   - å‰ 1000 ä¸ªï¼šè¿›å…¥ waiting_queue
#   - åé¢çš„ï¼šæ”¶åˆ° 503 é”™è¯¯
#     {
#       "error": {
#         "type": "abort",
#         "status_code": 503,
#         "message": "The request queue is full."
#       }
#     }
#
# æœåŠ¡å™¨çŠ¶æ€ï¼š
#   âœ… å†…å­˜å¯æ§ï¼ˆæœ€å¤š 1000 ä¸ªè¯·æ±‚åœ¨ç­‰å¾…ï¼‰
#   âœ… ä¸ä¼š OOM
#   âœ… ç»§ç»­å¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚
#   âœ… æ–°è¯·æ±‚è¢«æ‹’ç»ï¼Œä½†æœåŠ¡ä¸å´©æºƒ
```

---

## 9. å…³é”®é…ç½®å‚æ•°

### 9.1 å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ä½œç”¨ |
|-----|------|--------|------|
| `max_total_num_tokens` | int | è‡ªåŠ¨è®¡ç®— | KV cache æ€»å®¹é‡ |
| `max_prefill_tokens` | int | è‡ªåŠ¨è®¡ç®— | ä¸€æ¬¡ prefill batch çš„æœ€å¤§ tokens æ•° |
| `max_running_requests` | int | è‡ªåŠ¨è®¡ç®— | running_batch çš„æœ€å¤§è¯·æ±‚æ•° |
| `max_queued_requests` | int | None | waiting_queue çš„æœ€å¤§è¯·æ±‚æ•°ï¼ˆNone = æ— é™åˆ¶ï¼‰ |
| `chunked_prefill_size` | int | None | å•ä¸ªè¯·æ±‚çš„æœ€å¤§ tokens æ•°ï¼ˆNone = ç¦ç”¨ï¼‰ |
| `schedule_policy` | str | "lpm" | è°ƒåº¦ç­–ç•¥ï¼ˆfcfs/lpm/dfs-weight/lof/randomï¼‰ |

### 9.2 é…ç½®ç¤ºä¾‹

```python
from sglang import ServerArgs

# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
server_args = ServerArgs(
    model_path="meta-llama/Llama-2-7b-chat-hf",
    
    # æµé‡æ§åˆ¶
    max_queued_requests=2000,      # æœ€å¤šæ’é˜Ÿ 2000 ä¸ªè¯·æ±‚
    
    # æ‰¹å¤„ç†é…ç½®
    max_prefill_tokens=16384,      # ä¸€æ¬¡ prefill æœ€å¤š 16K tokens
    chunked_prefill_size=8192,     # å•ä¸ªè¯·æ±‚è¶…è¿‡ 8K å°±åˆ†å—
    max_running_requests=256,      # decode æ± å­æœ€å¤š 256 ä¸ªè¯·æ±‚
    
    # è°ƒåº¦ç­–ç•¥
    schedule_policy="lpm",         # æœ€é•¿å‰ç¼€åŒ¹é…ï¼ˆåˆ©ç”¨ KV cacheï¼‰
    enable_priority_scheduling=True,  # å¯ç”¨ä¼˜å…ˆçº§è°ƒåº¦
    
    # å†…å­˜ç®¡ç†
    mem_fraction_static=0.9,       # 90% GPU å†…å­˜ç”¨äº KV cache
)
```

---

## 10. å®Œæ•´æµç¨‹ç¤ºä¾‹

### 10.1 æ—¶é—´çº¿ç¤ºä¾‹

```
æ—¶åˆ» T0: ç³»ç»Ÿåˆå§‹åŒ–
  waiting_queue = []
  running_batch = []

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T1: 3 ä¸ªè¯·æ±‚åˆ°è¾¾
  HTTP POST â†’ TokenizerManager â†’ Scheduler
  waiting_queue = [Req1(100 tokens), Req2(200 tokens), Req3(150 tokens)]
  running_batch = []

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T2: ç¬¬ 1 è½®å¾ªç¯ - Prefill
  get_next_batch_to_run():
    â†’ get_new_batch_prefill()
    â†’ æ‰“åŒ… [Req1, Req2, Req3]
    â†’ new_batch.prepare_for_extend()
  
  run_batch(new_batch):
    â†’ ForwardMode.EXTEND
    â†’ GPU ä¸€æ¬¡æ€§å¤„ç† 450 tokens
    â†’ æ‰€æœ‰è¯·æ±‚ prefill å®Œæˆ
  
  process_batch_result():
    â†’ åˆå¹¶åˆ° running_batch
  
  waiting_queue = []
  running_batch = [Req1(decode), Req2(decode), Req3(decode)]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T3: ç¬¬ 2 è½®å¾ªç¯ - Decode
  get_next_batch_to_run():
    â†’ get_new_batch_prefill() = None (é˜Ÿåˆ—ç©º)
    â†’ update_running_batch(running_batch)
    â†’ running_batch.prepare_for_decode()
  
  run_batch(running_batch):
    â†’ ForwardMode.DECODE
    â†’ GPU ç”Ÿæˆ 3 ä¸ª tokens (æ¯ä¸ª req 1 ä¸ª)
  
  process_batch_result():
    â†’ filter_batch() â†’ æ‰€æœ‰éƒ½æœªå®Œæˆ
  
  running_batch = [Req1(1/50), Req2(1/10), Req3(1/100)]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T4-T11: ç¬¬ 3-10 è½®å¾ªç¯ - Decode
  æŒç»­ decode...
  
  æ–°è¯·æ±‚åˆ°è¾¾ï¼šReq4(120 tokens), Req5(80 tokens)
  waiting_queue = [Req4, Req5]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T12: ç¬¬ 11 è½®å¾ªç¯ - Req2 å®Œæˆ
  run_batch(running_batch):
    â†’ Req2 ç”Ÿæˆ EOS token
  
  process_batch_result():
    â†’ filter_batch()
    â†’ Req2.finished() = True
    â†’ ç§»é™¤ Req2 âœ…
    â†’ batch_is_full = False
  
  running_batch = [Req1(10/50), Req3(10/100)]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T13: ç¬¬ 12 è½®å¾ªç¯ - åŠ å…¥æ–°è¯·æ±‚
  get_next_batch_to_run():
    â†’ get_new_batch_prefill()
    â†’ æ‰“åŒ… [Req4, Req5] (200 tokens)
    â†’ new_batch.prepare_for_extend()
    â†’ new_batch.mix_with_running(running_batch) ğŸ”¥
  
  run_batch(new_batch):
    â†’ ForwardMode.MIXED  ğŸ”¥ æ··åˆæ¨¡å¼ï¼
    â†’ GPU åŒæ—¶å¤„ç†ï¼š
        - Req4 çš„ 120 tokens (prefill)
        - Req5 çš„ 80 tokens (prefill)
        - Req1 çš„ 1 token (decode)
        - Req3 çš„ 1 token (decode)
  
  process_batch_result():
    â†’ Req4, Req5 prefill å®Œæˆï¼Œè¿›å…¥ decode
    â†’ åˆå¹¶åˆ° running_batch
  
  waiting_queue = []
  running_batch = [Req1(11/50), Req3(11/100), Req4(1/20), Req5(1/30)]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ—¶åˆ» T14-...: ç»§ç»­è½®è¯¢ decode
  running_batch ä¸­çš„è¯·æ±‚é€ä¸ªå®Œæˆ
  æ–°è¯·æ±‚åˆ°è¾¾æ—¶ç«‹å³å¡«å……
  
  å¾ªç¯å¾€å¤ï¼ŒGPU æ°¸ä¸ç©ºé—²... ğŸ”„
```

### 10.2 Forward Mode è½¬æ¢å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Forward Mode çŠ¶æ€æœº                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ–°è¯·æ±‚åˆ°è¾¾ (waiting_queue éç©º)
    â†“
EXTEND (Prefill)
    â”œâ”€â†’ æ‰€æœ‰è¯·æ±‚ prefill å®Œæˆ â†’ åˆå¹¶åˆ° running_batch
    â””â”€â†’ æœ‰è¿è¡Œä¸­çš„ decode â†’ MIXED
              â†“
        MIXED (æ··åˆæ¨¡å¼)
            â”œâ”€â†’ prefill + decode åŒæ—¶å¤„ç†
            â””â”€â†’ prefill å®Œæˆï¼Œå…¨éƒ¨è¿›å…¥ decode
                      â†“
                DECODE (é€ token ç”Ÿæˆ)
                    â”œâ”€â†’ æ¯è½®ç”Ÿæˆ 1 token
                    â”œâ”€â†’ filter_batch() ç§»é™¤å®Œæˆçš„
                    â””â”€â†’ æœ‰æ–°è¯·æ±‚ â†’ MIXED
                              â†“
                        å¾ªç¯å¾€å¤...
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Continuous Batching çš„æœ¬è´¨**ï¼šç»´æŠ¤ä¸€ä¸ªåŠ¨æ€çš„è¯·æ±‚æ± ï¼Œæ¯è½® decode åç«‹å³ç§»é™¤å®Œæˆçš„è¯·æ±‚å¹¶åŠ å…¥æ–°è¯·æ±‚ã€‚

2. **Decode æ˜¯ä¸»æˆ˜åœº**ï¼šç”±äºé€ token ç”Ÿæˆã€æŒç»­æ—¶é—´é•¿ã€å®Œæˆæ—¶é—´ä¸åŒæ­¥ï¼Œdecode é˜¶æ®µæ˜¯ continuous batching æ”¶ç›Šæœ€å¤§çš„åœ°æ–¹ã€‚

3. **Prefill æ‰¹é‡æ‰“åŒ…**ï¼šå¤šä¸ªæ–°è¯·æ±‚ä¼šè¢«æ‰“åŒ…æˆä¸€ä¸ª batch å¹¶è¡Œå¤„ç†ï¼Œä½†é‡åˆ°èµ„æºé™åˆ¶ä¼šç«‹å³åœæ­¢ï¼ˆç¬¬ä¸€æ¬¡å¤±è´¥å°±åœï¼‰ã€‚

4. **ä¸€å£æ°” vs é€ token**ï¼š
   - Prefillï¼šä¸€å£æ°”å¤„ç†æ¯ä¸ªè¯·æ±‚çš„å…¨éƒ¨è¾“å…¥ tokens
   - Decodeï¼šæ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª tokenï¼Œéœ€è¦å¤šè½®è¿­ä»£

5. **ä¸¤ä¸ªç‹¬ç«‹æœºåˆ¶**ï¼š
   - `max_prefill_tokens`ï¼šé™åˆ¶ä¸€æ¬¡ batch çš„æ€» tokensï¼Œè¶…é™åˆ™å»¶å
   - `chunked_prefill_size`ï¼šé™åˆ¶å•ä¸ªè¯·æ±‚çš„ tokensï¼Œè¶…é™åˆ™åˆ†å—

6. **å®Œå–„çš„è¿‡è½½ä¿æŠ¤**ï¼šä¸‰å±‚é˜²æŠ¤æœºåˆ¶ç¡®ä¿ç–¯ç‹‚å‘è¯·æ±‚ä¹Ÿä¸ä¼šå´©æºƒï¼Œè¿”å› 503 è€Œä¸æ˜¯ OOMã€‚

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½® `max_prefill_tokens`**ï¼šå¤ªå°å¯¼è‡´ååé‡ä½ï¼Œå¤ªå¤§å¯¼è‡´å»¶è¿Ÿé«˜
2. **å¯ç”¨ `chunked_prefill_size`**ï¼šå¤„ç†è¶…é•¿è¾“å…¥æ—¶é¿å… OOM
3. **é…ç½® `max_queued_requests`**ï¼šé˜²æ­¢å†…å­˜æ— é™å¢é•¿
4. **é€‰æ‹©åˆé€‚çš„è°ƒåº¦ç­–ç•¥**ï¼š`lpm` å¯¹ç¼“å­˜å‹å¥½ï¼Œ`fcfs` å…¬å¹³
5. **å¯ç”¨ä¼˜å…ˆçº§è°ƒåº¦**ï¼šä¿æŠ¤é‡è¦è¯·æ±‚

### ä»£ç ä½ç½®ç´¢å¼•

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ | è¡Œæ•° |
|-----|---------|------|
| ä¸»äº‹ä»¶å¾ªç¯ | `python/sglang/srt/managers/scheduler.py` | 979-995 |
| æ›´æ–° running_batch | `python/sglang/srt/managers/scheduler.py` | 2016-2060 |
| è¿‡æ»¤å®Œæˆè¯·æ±‚ | `python/sglang/srt/managers/schedule_batch.py` | 1720-1757 |
| è·å– prefill batch | `python/sglang/srt/managers/scheduler.py` | 1835-2014 |
| PrefillAdder | `python/sglang/srt/managers/schedule_policy.py` | 315-698 |
| é˜Ÿåˆ—æº¢å‡ºä¿æŠ¤ | `python/sglang/srt/managers/scheduler.py` | 1528-1568 |
| Forward Mode | `python/sglang/srt/model_executor/forward_batch_info.py` | 62-129 |

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šåŸºäº sglang æºç åˆ†æï¼ˆ2025-01ï¼‰  
**ä½œè€…**ï¼šAI Assistant  
**æœ€åæ›´æ–°**ï¼š2025-01-10

